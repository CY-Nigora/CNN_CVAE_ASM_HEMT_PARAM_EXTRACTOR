{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8ba190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/usr/bin/env python3\n",
    "# Use other Python kernel other than the in ADS integrated one\n",
    "# to provide more flexibility in the code\n",
    "# and to avoid the limitation of the ADS integrated Python kernel\n",
    "\n",
    "''' File: data_pre_processing.ipynb:\n",
    "    This file is used to modify the generated data from data_gen.ipynb to finally match the format required by further DNN training by PyTorch.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.signal import savgol_filter\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78356bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Loading ...\n",
    "\n",
    "def smooth_vec(y: np.ndarray) -> np.ndarray:\n",
    "        '''use savgol_filter to smooth input vector'''\n",
    "        n = y.size\n",
    "        if n < 5:\n",
    "            return y\n",
    "        win = min(11, n if n % 2 == 1 else n - 1)\n",
    "        win = max(win, 5)\n",
    "        return savgol_filter(y, window_length=win, polyorder=2, mode='interp')\n",
    "\n",
    "\n",
    "def error_gain_calc(meas_data_selected: np.ndarray, meas_data_smooth: np.ndarray):\n",
    "    \n",
    "    g1_matrix = np.matmul(meas_data_selected, meas_data_smooth.T)\n",
    "    g2_matrix = np.matmul(meas_data_smooth, meas_data_smooth.T)\n",
    "    g = np.sum(np.diag(g1_matrix)) / np.sum(np.diag(g2_matrix))\n",
    "    error_gain = g - 1\n",
    "\n",
    "    return error_gain, g\n",
    "\n",
    "\n",
    "def alpha_beta_calc(Id_meas, Id_base, g,\n",
    "                   n_bins=20, min_count=10,\n",
    "                   log_bins=True, robust=False):\n",
    "\n",
    "    Id_meas = np.asarray(Id_meas, float)\n",
    "    Id_base = np.asarray(Id_base, float)\n",
    "    if Id_meas.shape != Id_base.shape:\n",
    "        raise ValueError(\"Id_meas 与 Id_base 形状必须一致\")\n",
    "\n",
    "    # 1) 残差与参考横轴\n",
    "    ref = np.abs(g * Id_base).ravel()\n",
    "    r   = (Id_meas - g * Id_base).ravel()\n",
    "\n",
    "    good = np.isfinite(ref) & np.isfinite(r)\n",
    "    ref = ref[good]; r = r[good]\n",
    "    if ref.size < 2:\n",
    "        raise ValueError(\"有效样本太少，无法拟合\")\n",
    "\n",
    "    # 2) 生成分箱（优先对数分箱；动态范围不足则退化为线性分箱）\n",
    "    eps = 1e-15\n",
    "    use_log = False\n",
    "    if log_bins:\n",
    "        pos = ref[ref > 0]\n",
    "        if pos.size >= 10:\n",
    "            lo, hi = np.percentile(pos, 5), np.percentile(pos, 95)\n",
    "            if hi / max(lo, eps) >= 10:   # 至少一阶量级\n",
    "                use_log = True\n",
    "                lo = max(lo, eps)\n",
    "                edges = np.logspace(np.log10(lo), np.log10(hi), n_bins + 1)\n",
    "    if not use_log:\n",
    "        lo, hi = np.percentile(ref, 5), np.percentile(ref, 95)\n",
    "        edges = np.linspace(lo, hi, n_bins + 1)\n",
    "\n",
    "    idx = np.digitize(ref, edges, right=False)  # 1..n_bins\n",
    "\n",
    "    # 3) 每个箱的 σ（std 或 MAD）\n",
    "    centers, sigmas, counts = [], [], []\n",
    "    for b in range(1, len(edges)):\n",
    "        m = (idx == b)\n",
    "        c = int(np.count_nonzero(m))\n",
    "        if c < min_count:\n",
    "            continue\n",
    "        center = np.median(ref[m])\n",
    "        vals   = r[m]\n",
    "        if robust:\n",
    "            med = np.median(vals)\n",
    "            sigma = 1.4826 * np.median(np.abs(vals - med))  # MAD→std\n",
    "        else:\n",
    "            sigma = vals.std(ddof=1)\n",
    "        if np.isfinite(sigma) and sigma > 0:\n",
    "            centers.append(center); sigmas.append(sigma); counts.append(c)\n",
    "\n",
    "    centers = np.asarray(centers); sigmas = np.asarray(sigmas); counts = np.asarray(counts)\n",
    "    if centers.size < 2:\n",
    "        raise ValueError(\"有效分箱不足，调小 min_count 或增大 n_bins\")\n",
    "\n",
    "    # 4) 加权最小二乘拟合: sigma ≈ alpha * I + beta\n",
    "    A = np.column_stack([centers, np.ones_like(centers)])\n",
    "    w = np.sqrt(counts)                         # 权重≈置信度\n",
    "    Aw = A * w[:, None]\n",
    "    yw = sigmas * w\n",
    "    (alpha, beta), *_ = np.linalg.lstsq(Aw, yw, rcond=None)\n",
    "\n",
    "    # R^2 诊断\n",
    "    yhat   = A @ np.array([alpha, beta])\n",
    "    ss_res = float(np.sum((sigmas - yhat)**2))\n",
    "    ss_tot = float(np.sum((sigmas - sigmas.mean())**2))\n",
    "    r2     = 1 - ss_res / ss_tot if ss_tot > 0 else np.nan\n",
    "\n",
    "    diag = {\n",
    "        \"R2\": r2,\n",
    "        \"centers\": centers,\n",
    "        \"sigmas\": sigmas,\n",
    "        \"counts\": counts,\n",
    "        \"edges\": edges,\n",
    "        \"used_log_bins\": use_log,\n",
    "    }\n",
    "    return alpha, beta, diag\n",
    "\n",
    "\n",
    "\n",
    "def noise_coefficient_estimation(meas_file: str, X_shape: dict, rename: list=None, plot_flag: bool=True, save_name: str=None):\n",
    "    '''\n",
    "    Read meas data from .csv and return numpy, \n",
    "    also maps range into Vds in [-3.5, 8.5],\n",
    "    with step size of 0.1,\n",
    "    estimate 3 paramters in order to generate noise on simulated data\n",
    "    '''\n",
    "\n",
    "    # >> read data from measurement file\n",
    "    df = pd.read_csv(meas_file)\n",
    "    if rename is None:\n",
    "        row_name = df.columns[0]\n",
    "        col_name = df.columns[1]\n",
    "        value_name = df.columns[2]\n",
    "    else:\n",
    "        [row_name, col_name, value_name] = rename\n",
    "        df = df.rename(columns={df.columns[0]: row_name, df.columns[1]: col_name, df.columns[2]: value_name})\n",
    "    print(f'【CHECK】in current meas-data file:')\n",
    "    print(f'\\t row name is {row_name}, col name is {col_name}, value name is {value_name}')\n",
    "\n",
    "\n",
    "    # >> validate dataset\n",
    "    row_unique_value = df[row_name].unique()\n",
    "    row_shape_meas_data = set()\n",
    "    for index,value in enumerate(row_unique_value):\n",
    "        row_shape_meas_data.add(df[df[row_name] == value].shape)\n",
    "    if len(row_shape_meas_data) != 1:\n",
    "        print(f'different sizes in each row detected: {row_shape_meas_data}')\n",
    "    else:\n",
    "        print(f'same size in each row detected: {len(row_unique_value)}*{row_shape_meas_data}')\n",
    "    if (len(row_unique_value)) != X_shape['row_num'] :\n",
    "        raise ValueError(f'The number of different rows is given {X_shape['row_num']} but is actually {len(row_unique_value)} with {row_unique_value.tolist()}')\n",
    "\n",
    "\n",
    "    # >> transform data to numpy and reshape + range mapping\n",
    "    meas_data_selected = np.zeros((X_shape['row_num'], X_shape['col_num']), dtype=np.float64)\n",
    "    meas_data_smooth = meas_data_selected.copy()\n",
    "    meas_data_save = np.zeros((1, X_shape['row_num'], X_shape['col_num']), dtype=np.float64)\n",
    "    row_unique_value.tolist().sort(reverse=False)\n",
    "    for index, row_value in enumerate(row_unique_value):\n",
    "        new_col_range = np.linspace(min(X_shape['col_range']), max(X_shape['col_range']), X_shape['col_num'])\n",
    "        old_col_range = df[df[row_name] == row_value][col_name].to_numpy()\n",
    "        old_value_range = df[df[row_name] == row_value][value_name].to_numpy()\n",
    "        meas_data_selected[index,:] = np.interp(new_col_range, old_col_range, old_value_range)\n",
    "        meas_data_smooth[index,:] = smooth_vec(meas_data_selected[index,:])\n",
    "\n",
    "\n",
    "    # >> params regression - least squares scaling\n",
    "    error_gain, g, alpha, beta = None, None, None, None\n",
    "\n",
    "    # calculate error_gain\n",
    "    error_gain, g = error_gain_calc(meas_data_selected, meas_data_smooth)\n",
    "\n",
    "    # calculate alpha, beta\n",
    "    meas_data_error = meas_data_selected - (g * meas_data_smooth)\n",
    "    std_vector = np.sqrt(np.diag(np.matmul(meas_data_error.T, meas_data_error))/X_shape['row_num'])\n",
    "\n",
    "    # alpha, beta = np.polyfit(new_col_range, std_vector, deg=1)\n",
    "\n",
    "    alpha, beta, dialog = alpha_beta_calc( Id_meas=meas_data_selected,\n",
    "                                        Id_base=meas_data_smooth,\n",
    "                                        g = g ) # TODO: solve this part\n",
    "\n",
    "    # optional, save measurement data with ruled format\n",
    "    if save_name:\n",
    "        # meas_data_save = meas_data_selected.copy()\n",
    "        meas_data_save[0,:,:] = meas_data_smooth.copy()\n",
    "\n",
    "        # check if dir exist\n",
    "        split_index = meas_file[::-1].find('/')\n",
    "        path = meas_file[:(- split_index)] + save_name\n",
    "\n",
    "        with h5py.File(path, 'w') as f:\n",
    "            f.create_dataset('X', data=meas_data_save)\n",
    "            print(f'sucessfully save measurement data under path {path}, with shape of {meas_data_selected.shape}')\n",
    "        \n",
    "\n",
    "    # optional, plot to check performance\n",
    "    if plot_flag:\n",
    "\n",
    "         # test plot 1 - original data vs imterploted data vs smoothed data\n",
    "        _, ax = plt.subplots(2,2, figsize=(18, 13))\n",
    "        for index, row_value in enumerate(row_unique_value):\n",
    "            range1 = df[row_name] == row_value\n",
    "            range2 = df[col_name] >= X_shape['col_range'][0]\n",
    "            range3 = df[col_name] <= X_shape['col_range'][1]\n",
    "            range = range1 & range2 & range3\n",
    "            # plot only within defined given col range\n",
    "            ax[0,0].plot(df[range][col_name], df[range][value_name], label = f\"{row_name} = {row_value} V\")\n",
    "            ax[0,1].plot(new_col_range, meas_data_selected[index,:], label = f\"{row_name} = {row_value} V\")\n",
    "            ax[1,0].plot(new_col_range, smooth_vec(meas_data_selected[index,:]), label = f\"{row_name} = {row_value} V\")\n",
    "            # ax[1,1].plot(new_col_range, meas_data_selected[index,:] - smooth_vec(meas_data_selected[index,:]), label = f\"{row_name} = {row_value} V\")\n",
    "            ax[1,1].plot(df[range][col_name], df[range][value_name], label = f\"{row_name} = {row_value} V\", linestyle = '--')\n",
    "            ax[1,1].plot(new_col_range, smooth_vec(meas_data_selected[index,:]), label = f\"{row_name} = {row_value} V\")\n",
    "\n",
    "        ax[0,0].set_title('test plot of original meas data')\n",
    "        ax[0,1].set_title('test plot of interplotted data')\n",
    "        ax[1,0].set_title('test plot of smoothed data')\n",
    "        # ax[1,1].set_title('error between interplotted adn sommthed data')\n",
    "        ax[1,1].set_title('smoothed v.s. original data')\n",
    "        for i, j in np.ndindex((2,2)):\n",
    "            ax[i,j].set_xlabel(f\"{col_name}\")\n",
    "            ax[i,j].set_ylabel(f\"{value_name}\")\n",
    "            ax[i,j].legend()\n",
    "            ax[i,j].grid()\n",
    "        plt.show()\n",
    "\n",
    "        # test plot 2 - regression of alpha and beta\n",
    "        _, ax = plt.subplots(figsize=(9, 6.5))\n",
    "        ax.plot(new_col_range, std_vector, 'x', label = 'std of errors')\n",
    "        ax.plot(new_col_range, new_col_range * alpha + beta,'--', label='regression curve')\n",
    "        ax.set_title('test plot of original meas data')\n",
    "        ax.set_xlabel(f\"{col_name}\")\n",
    "        ax.set_ylabel(f\"Standard Deviation of {value_name}\")\n",
    "        ax.legend()\n",
    "        ax.grid()\n",
    "         \n",
    "    return error_gain, g, alpha, beta\n",
    "\n",
    "\n",
    "# filter rule 1: max value of each sample shoudl be at least > x, to remove meaningless noise\n",
    "# filter rule 2: with risen Vds, max Ids must also rise\n",
    "def filter_mask(X: np.ndarray, mask_1_flag = False, mask_2_flag = False, threshold:float = 2) -> np.ndarray:\n",
    "\n",
    "    X = np.asarray(X)\n",
    "    mask_2 = np.array([True]*X.shape[0]).reshape(-1,1)  # default all True\n",
    "    mask_1 = mask_2.copy()\n",
    "    # assert X.ndim == 3 and X.shape[1] == 7 and X.shape[2] == 121\n",
    "    #  -> (N, 7)\n",
    "    last_col = X[:, :, -1]\n",
    "\n",
    "    if mask_1_flag:\n",
    "        max_per_sample = last_col.max(axis=1, keepdims=True).reshape(-1,1)\n",
    "        mask_1 = (max_per_sample > threshold) # for output > 2, transfer > 0.05\n",
    "\n",
    "    if mask_2_flag:\n",
    "        diffs = np.diff(last_col, axis=1) # attention here, data in each sample is of ascending order\n",
    "        mask_2 = np.all(diffs > 0, axis=1).reshape(-1,1)\n",
    "\n",
    "    mask = mask_1 & mask_2\n",
    "\n",
    "    return mask\n",
    "\n",
    "# filter rule 1: max value of each sample shoudl be at least > x, to remove meaningless noise\n",
    "# filter rule 2: with risen Vds, max Ids must also rise (soft version: allow small / few violations)\n",
    "def soft_filter_mask(X: np.ndarray, \n",
    "                     mask_1_flag: bool = False, mask_2_flag: bool = False, \n",
    "                     threshold: float = 2.0, eps_small: float = 1e-2) -> np.ndarray:\n",
    "\n",
    "    X = np.asarray(X)\n",
    "    N = X.shape[0]\n",
    "\n",
    "    # default: keep all\n",
    "    mask_1 = np.ones((N, 1), dtype=bool)\n",
    "    mask_2 = np.ones((N, 1), dtype=bool)\n",
    "    last_col = X[:, :, -1]\n",
    "\n",
    "    # --------- filter1: global max threshold ----------\n",
    "    if mask_1_flag:\n",
    "        max_per_sample = last_col.max(axis=1, keepdims=True)  # (N, 1)\n",
    "        mask_1 = (max_per_sample > threshold)                 # (N, 1)\n",
    "\n",
    "    # --------- filter2: soft monotonicity on last_col vs. row index ----------\n",
    "    if mask_2_flag:\n",
    "        diffs = np.diff(last_col, axis=1)   # shape: (N, n_rows-1)\n",
    "\n",
    "        # Tolerate extremely small numerical fluctuations: \n",
    "        # Only consider \"significant decreases\" as violations of monotonicity\n",
    "        # eps_small: Numerical fluctuation threshold; ignore values ​​less than eps_small.\n",
    "\n",
    "        bad_steps = diffs <= (-1)*eps_small        # (N, n_rows-1) bool\n",
    "        num_steps = diffs.shape[1]\n",
    "        num_bad = bad_steps.sum(axis=1)       \n",
    "        frac_bad = num_bad / max(1, num_steps)\n",
    "\n",
    "        # The magnitude of the decline (only looking at the points of decline)\n",
    "        drop_mag = np.where(bad_steps, -diffs, 0.0)   # A decrease is positive, otherwise it is 0.\n",
    "        max_drop_abs = drop_mag.max(axis=1)           # Maximum decrease for each sample\n",
    "\n",
    "        # Relative magnitude (relative to the maximum Ids of this sample)\n",
    "        per_sample_max = last_col.max(axis=1)\n",
    "        max_drop_rel = max_drop_abs / np.clip(per_sample_max, 1e-6, None)\n",
    "\n",
    "        # ---- Soft Rule: Allows small and minor decreases ----\n",
    "        # 1) A maximum of 10% of the step size is allowed to be a \"significant decrease\".\n",
    "        max_frac_bad = 0.10\n",
    "        # 2) The maximum allowable drop is a peak value of <= 10% (equivalent to a small local fluctuation).\n",
    "        max_rel_drop = 0.10\n",
    "\n",
    "        good = (frac_bad <= max_frac_bad) & (max_drop_rel <= max_rel_drop)\n",
    "        mask_2 = good.reshape(-1, 1)\n",
    "\n",
    "    mask = mask_1 & mask_2\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c206c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''STEP 1 : Variable INIT'''\n",
    "\n",
    "# ---------------------------------PATH-------------------------\n",
    "\n",
    "# path of saved .h5 data\n",
    "# generated data path, the data adding noise will be saved in sub folder\n",
    "data_save_path = \"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/data/log_Bidi_dataset/dataGen2\"\n",
    "noisy_data_path = data_save_path + '/noisy_data'\n",
    "\n",
    "# path of measured data\n",
    "meas_data_path = r\"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/data_pre_processing/ref_data\"\n",
    "meas_file_name = \"meas_data_Bidi_Transfer.csv\"\n",
    "meas_file = meas_data_path + '/' + meas_file_name\n",
    "\n",
    "meas_corrected_data_name = 'meas_smoothed_Bidi_Transfer.h5'\n",
    "# ----------------------------------VAR-------------------------\n",
    "num_set = 100 # here only 10 process/dataset available\n",
    "num_samples_single_set = 1000 # in each process/dataset 100 samples/simulations available\n",
    "\n",
    "row_X = 6\n",
    "col_X = 101\n",
    "row_Y = 11\n",
    "col_Y = 1\n",
    "row_range = (1, 6)\n",
    "col_range = (-10,0)\n",
    "goal_name = 'X_gm'  # 'X_iv' or 'X_gm'\n",
    "\n",
    "\n",
    "X_shape = {\n",
    "    \"row_num\": row_X,\n",
    "    \"col_num\": col_X,\n",
    "    \"row_range\": row_range,\n",
    "    \"col_range\": col_range\n",
    "}\n",
    "Y_shape = {\n",
    "    \"row_num\": row_Y,\n",
    "    \"col_num\": col_Y\n",
    "}\n",
    "rename_meas = ['Vds', 'Vgs', 'Ids'] if goal_name == 'X_gm' else ['Vgs', 'Vds', 'Ids']   # or None: keep original name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3d5b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''STEP 2 : calculate Noise Coefficient'''\n",
    "\n",
    "plot_validation_flag = True\n",
    "validation_data_save_name = meas_corrected_data_name\n",
    "\n",
    "error_gain, g, alpha, beta = noise_coefficient_estimation(meas_file, X_shape, rename_meas, plot_validation_flag, validation_data_save_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f327d926",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''STEP 3 : add noice in simulation data'''\n",
    "\n",
    "# pre-define matrixs\n",
    "X_goal = np.zeros((num_set*num_samples_single_set, X_shape['row_num'], X_shape['col_num']),dtype=np.float64)\n",
    "Y = np.zeros((num_set*num_samples_single_set, Y_shape['row_num'], Y_shape['col_num']),dtype=np.float64)\n",
    "\n",
    "# trans data from .h5 to numpy\n",
    "cur_index = 0\n",
    "for process_id in range(1, num_set + 1):\n",
    "    with h5py.File(f\"{data_save_path}\\\\dataset_process_{process_id}.h5\", 'r') as f:\n",
    "        index_update = f[goal_name].shape[0]\n",
    "        X_goal[cur_index : cur_index + index_update, :, :] = f[goal_name]\n",
    "        Y[cur_index : cur_index + index_update, :, :] = f['Y']\n",
    "        cur_index += index_update\n",
    "\n",
    "# remove unfilled part\n",
    "X_goal = X_goal[:cur_index]\n",
    "Y = Y[:cur_index]\n",
    "\n",
    "# generated data is in descending order of row, need to be reversed\n",
    "X_goal = X_goal[:,::-1,:] \n",
    "\n",
    "# add noise in simulation I-V data\n",
    "sigma_matrix = np.abs(alpha * X_goal + beta)\n",
    "noise = np.random.default_rng().normal(loc=0.0, scale=sigma_matrix)\n",
    "X_goal_noisy = g * X_goal + noise\n",
    "# X_goal_noisy = X_goal\n",
    "\n",
    "# remove outliers\n",
    "# filter rule 1: max value of each sample shoudl be at least > x, to remove meaningless noise\n",
    "# filter rule 2: with risen Vds, max Ids must also rise\n",
    "rule_1_flag = True\n",
    "rule_2_flag = True\n",
    "outliner_mask = filter_mask(X_goal_noisy, rule_1_flag, rule_2_flag)\n",
    "X_goal_noisy_outlier_removal = X_goal_noisy[outliner_mask.reshape(-1),:,:]\n",
    "Y_outlier_removal = Y[outliner_mask.reshape(-1),:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d45e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''STEP x : Gen 2-stage dataset'''\n",
    "\n",
    "dataset_1 = \"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/data_gen/data/bigger_dataset/noisy_data/train_bigger_dataset_filter_7row.h5\"\n",
    "dataset_2 = \"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/data_gen/data/normal_dataset/noisy_data/train_dataset_7row.h5\"\n",
    "dataset_3 = \"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/data_gen/data/log_normal_dataset/noisy_data/train_log_dataset_7row_2.h5\"\n",
    "dataset_4 = \"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/data_gen/data/log_normal_dataset/noisy_data/train_log_dataset_7row_1.h5\"\n",
    "\n",
    "\n",
    "f1 = h5py.File(dataset_1, 'r')\n",
    "f2 = h5py.File(dataset_2, 'r')\n",
    "f3 = h5py.File(dataset_3, 'r')\n",
    "f4 = h5py.File(dataset_4, 'r')\n",
    "\n",
    "print('f1-x shape: ',f1['X'].shape)\n",
    "print('f2-x shape: ',f2['X'].shape)\n",
    "print('f3-x shape: ',f3['X'].shape)\n",
    "print('f4-x shape: ',f4['X'].shape)\n",
    "print('f-x type: ',type(f3['X']))\n",
    "\n",
    "f_X = np.vstack((f1['X'],f2['X'],f3['X']))\n",
    "f_Y = np.vstack((f1['Y'],f2['Y'],f3['Y']))\n",
    "\n",
    "meas_like_X = f4['X']\n",
    "meas_like_Y = f4['Y']\n",
    "\n",
    "print('f_X shape: ',f_X.shape)\n",
    "print('f_Y shape: ',f_Y.shape)\n",
    "\n",
    "print('meas_like_X shape: ',meas_like_X.shape)\n",
    "print('meas_like_Y shape: ',meas_like_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269cc0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_1_flag = True\n",
    "rule_2_flag = True\n",
    "outliner_mask = filter_mask(f_X, rule_1_flag, rule_2_flag)\n",
    "meas_like_outliner_mask = filter_mask(meas_like_X, rule_1_flag, rule_2_flag)\n",
    "\n",
    "train_X = f_X[outliner_mask.reshape(-1),:,:]\n",
    "train_Y = f_Y[outliner_mask.reshape(-1),:,:]\n",
    "\n",
    "meas_like_X = meas_like_X[meas_like_outliner_mask.reshape(-1),:,:]\n",
    "meas_like_Y = meas_like_Y[meas_like_outliner_mask.reshape(-1),:,:]\n",
    "\n",
    "print('shape of full training set X: ',train_X.shape)\n",
    "print('shape of full training set Y: ',train_Y.shape)\n",
    "\n",
    "print('shape of meas like fine-turning set X: ',meas_like_X.shape)\n",
    "print('shape of meas like fine-turning set Y: ',meas_like_Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a799988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_save_path = \"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/NN_training/dataset/training/stage_1_full_data.h5\"\n",
    "meas_like_data_save_path = \"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/NN_training/dataset/training/stage_2_ml_data.h5\"\n",
    "full_plus_ml_data_save_path = \"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/NN_training/dataset/training/stage_1_fpml_data.h5\"\n",
    "\n",
    "with h5py.File(full_data_save_path, 'w') as f:\n",
    "    f.create_dataset('X', data=train_X)\n",
    "    f.create_dataset('Y', data=train_Y)\n",
    "\n",
    "with h5py.File(meas_like_data_save_path, 'w') as f:\n",
    "    f.create_dataset('X', data=meas_like_X)\n",
    "    f.create_dataset('Y', data=meas_like_Y)\n",
    "\n",
    "with h5py.File(full_plus_ml_data_save_path, 'w') as f:\n",
    "    f.create_dataset('X', data=np.vstack((train_X, meas_like_X)))\n",
    "    f.create_dataset('Y', data=np.vstack((train_Y, meas_like_Y)))\n",
    "\n",
    "f1.close()\n",
    "f2.close()\n",
    "f3.close()\n",
    "f4.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7865197",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_goal_noisy.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "\n",
    "print(X_goal_noisy.shape, X_goal_noisy_outlier_removal.shape)\n",
    "print(Y.shape, Y_outlier_removal.shape)\n",
    "# print(np.where(outliner_mask==False)[0])\n",
    "print(np.where(outliner_mask==False)[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aebcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''STEP 4 : save noisy data as torch'''\n",
    "\n",
    "# check if dir exist\n",
    "if not os.path.exists(noisy_data_path):\n",
    "    os.makedirs(noisy_data_path)\n",
    "    print('dir not found, create new dir based on given path')\n",
    "else:\n",
    "    ('path exists')\n",
    "print('saving data ...')\n",
    "\n",
    "file_name = 'Bidi_Transfer_clean_log.h5'\n",
    "path = noisy_data_path + '/' + file_name\n",
    "with h5py.File(path, 'w') as f:\n",
    "    f.create_dataset('X', data=X_goal_noisy_outlier_removal)\n",
    "    f.create_dataset('Y', data=Y_outlier_removal)\n",
    "\n",
    "\n",
    "print('data saved !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ded2720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test :: build a classifier to findout the data matching the distribution of meas data\n",
    "# remove outliers\n",
    "max_per_sample = np.max(X_iv_noisy_outlier_removal, axis=(1,2)).reshape(-1,1)\n",
    "ratio_vector = np.zeros((max_per_sample.shape[0],1),dtype=np.float64)\n",
    "for index in range(max_per_sample.shape[0]):\n",
    "    ratio_vector[index] = np.mean(np.diff(X_iv_noisy_outlier_removal[index, :, -1],axis = 0)) / (max_per_sample[index, 0] / 6)\n",
    "\n",
    "positive_ratio_index = ratio_vector > 0\n",
    "\n",
    "print(f'ratio is {ratio_vector}')\n",
    "print(f'positive ratio index is {np.where(positive_ratio_index==True)[0]}', f'with size of {np.where(positive_ratio_index==True)[0].shape}')\n",
    "print(np.max(ratio_vector), np.where(ratio_vector==np.max(ratio_vector))[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7754b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test :: merge multiple different .h5 datasets\n",
    "\n",
    "file_name_1 = 'E:\\\\personal_Data\\\\Document of School\\\\Uni Stuttgart\\\\Masterarbeit\\\\Code\\\\param_regression\\\\ADS_Parameter_Fitting\\\\IV_param_regression\\\\data\\\\log_Bidi_dataset\\\\dataGen1\\\\noisy_data\\\\Bidi_Transfer_clean_log.h5'\n",
    "\n",
    "file_name_2 = 'E:\\\\personal_Data\\\\Document of School\\\\Uni Stuttgart\\\\Masterarbeit\\\\Code\\\\param_regression\\\\ADS_Parameter_Fitting\\\\IV_param_regression\\\\data\\\\log_Bidi_dataset\\\\dataGen2\\\\noisy_data\\\\Bidi_Transfer_clean_log.h5'\n",
    "\n",
    "# file_name_3 = 'E:\\\\personal_Data\\\\Document of School\\\\Uni Stuttgart\\\\Masterarbeit\\\\Code\\\\param_regression\\\\ADS_Parameter_Fitting\\\\IV_param_regression\\\\data\\\\log_Bidi_dataset\\\\dataGen3\\\\noisy_data\\\\Bidi_Transfer_log.h5'\n",
    "\n",
    "save_path = 'E:\\\\personal_Data\\\\Document of School\\\\Uni Stuttgart\\\\Masterarbeit\\\\Code\\\\param_regression\\\\ADS_Parameter_Fitting\\\\IV_param_regression\\\\data\\\\log_Bidi_dataset\\\\data_merge'\n",
    "\n",
    "merge_file_name = 'Bidi_Transfer_clean_log_merge_2.h5'\n",
    "\n",
    "f1 = h5py.File(file_name_1, 'r')\n",
    "f2 = h5py.File(file_name_2, 'r')\n",
    "# f3 = h5py.File(file_name_3, 'r')\n",
    "\n",
    "# match size\n",
    "f_iterator = [f1,f2]\n",
    "f_col = set(i['X'].shape[2] for i in f_iterator)\n",
    "for index,f in enumerate(f_iterator):\n",
    "    print(f\"before correction: f{index+1}['X'].shape = {f['X'].shape}, f{index+1}['Y'].shape = {f['Y'].shape}\")\n",
    "\n",
    "# merge data\n",
    "\n",
    "if len(f_col) != 1:\n",
    "    print(f'[Error] col-num not matched !!!')\n",
    "else:\n",
    "    data_x = np.array(f_iterator[0]['X'])\n",
    "    data_y = np.array(f_iterator[0]['Y'])\n",
    "    for i in range(1,len(f_iterator)):\n",
    "        data_x = np.vstack((data_x, f_iterator[i]['X']))\n",
    "        data_y = np.vstack((data_y, f_iterator[i]['Y']))\n",
    "\n",
    "print(f\"after correction: data['X'].shape = {data_x.shape}, data['Y'].shape = {data_y.shape}\")\n",
    "\n",
    "# save data\n",
    "path = save_path + '/' + merge_file_name\n",
    "with h5py.File(path, 'w') as f:\n",
    "    f.create_dataset('X', data=data_x)\n",
    "    f.create_dataset('Y', data=data_y)\n",
    "\n",
    "print('data saved !')\n",
    "for f in f_iterator:\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7fac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "meas_path = \"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/data_pre_processing/data/meas_file_reshape_7row.h5\"\n",
    "save_path = \"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/data_pre_processing/data/temp.h5\"\n",
    "\n",
    "f = h5py.File(meas_path, 'r')\n",
    "print(f.keys())\n",
    "print(f['X'])\n",
    "\n",
    "temp = np.array(f['X'][:]).reshape(1,7,121)\n",
    "print(temp.shape)\n",
    "f.close()\n",
    "\n",
    "with h5py.File(save_path, 'w') as f:\n",
    "    f.create_dataset('X', data=temp)\n",
    "    print(f.keys())\n",
    "    print(f['X'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023be0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test plot\n",
    "\n",
    "index = 233\n",
    "index -= 1\n",
    "\n",
    "\n",
    "path = \"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/data/log_Bidi_dataset/dataGen1/noisy_data/Bidi_Transfer_clean_log.h5\"\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8)) \n",
    "with h5py.File(path, 'r') as f:\n",
    "    x_axis = np.linspace(min(X_shape['col_range']), max(X_shape['col_range']), f['X'].shape[2])\n",
    "    row_list = np.linspace(min(X_shape['row_range']), max(X_shape['row_range']), f['X'].shape[1])\n",
    "    for i in range(f['X'].shape[1]):\n",
    "        ax.plot(x_axis, f['X'][index, i, :], label=f'V_row = {row_list[i]}')\n",
    "        ax.grid(True)\n",
    "        ax.set_title('noisy I-V')\n",
    "        ax.set_xlabel(\"V_col\")\n",
    "        ax.set_ylabel(\"Ids\")\n",
    "        ax.legend()\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
