{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa8ba190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/usr/bin/env python3\n",
    "# Use other Python kernel other than the in ADS integrated one\n",
    "# to provide more flexibility in the code\n",
    "# and to avoid the limitation of the ADS integrated Python kernel\n",
    "\n",
    "''' File: data_pre_processing.ipynb:\n",
    "    This file is used to modify the generated data from data_gen.ipynb to finally match the format required by further DNN training by PyTorch.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.signal import savgol_filter\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78356bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Loading ...\n",
    "\n",
    "def smooth_vec(y: np.ndarray) -> np.ndarray:\n",
    "        '''use savgol_filter to smooth input vector'''\n",
    "        n = y.size\n",
    "        if n < 5:\n",
    "            return y\n",
    "        win = min(11, n if n % 2 == 1 else n - 1)\n",
    "        win = max(win, 5)\n",
    "        return savgol_filter(y, window_length=win, polyorder=2, mode='interp')\n",
    "\n",
    "\n",
    "def error_gain_calc(meas_data_selected: np.ndarray, meas_data_smooth: np.ndarray):\n",
    "    \n",
    "    g1_matrix = np.matmul(meas_data_selected, meas_data_smooth.T)\n",
    "    g2_matrix = np.matmul(meas_data_smooth, meas_data_smooth.T)\n",
    "    g = np.sum(np.diag(g1_matrix)) / np.sum(np.diag(g2_matrix))\n",
    "    error_gain = g - 1\n",
    "\n",
    "    return error_gain, g\n",
    "\n",
    "\n",
    "def alpha_beta_calc(Id_meas, Id_base, g,\n",
    "                   n_bins=20, min_count=10,\n",
    "                   log_bins=True, robust=False):\n",
    "\n",
    "    Id_meas = np.asarray(Id_meas, float)\n",
    "    Id_base = np.asarray(Id_base, float)\n",
    "    if Id_meas.shape != Id_base.shape:\n",
    "        raise ValueError(\"Id_meas 与 Id_base 形状必须一致\")\n",
    "\n",
    "    # 1) 残差与参考横轴\n",
    "    ref = np.abs(g * Id_base).ravel()\n",
    "    r   = (Id_meas - g * Id_base).ravel()\n",
    "\n",
    "    good = np.isfinite(ref) & np.isfinite(r)\n",
    "    ref = ref[good]; r = r[good]\n",
    "    if ref.size < 2:\n",
    "        raise ValueError(\"有效样本太少，无法拟合\")\n",
    "\n",
    "    # 2) 生成分箱（优先对数分箱；动态范围不足则退化为线性分箱）\n",
    "    eps = 1e-15\n",
    "    use_log = False\n",
    "    if log_bins:\n",
    "        pos = ref[ref > 0]\n",
    "        if pos.size >= 10:\n",
    "            lo, hi = np.percentile(pos, 5), np.percentile(pos, 95)\n",
    "            if hi / max(lo, eps) >= 10:   # 至少一阶量级\n",
    "                use_log = True\n",
    "                lo = max(lo, eps)\n",
    "                edges = np.logspace(np.log10(lo), np.log10(hi), n_bins + 1)\n",
    "    if not use_log:\n",
    "        lo, hi = np.percentile(ref, 5), np.percentile(ref, 95)\n",
    "        edges = np.linspace(lo, hi, n_bins + 1)\n",
    "\n",
    "    idx = np.digitize(ref, edges, right=False)  # 1..n_bins\n",
    "\n",
    "    # 3) 每个箱的 σ（std 或 MAD）\n",
    "    centers, sigmas, counts = [], [], []\n",
    "    for b in range(1, len(edges)):\n",
    "        m = (idx == b)\n",
    "        c = int(np.count_nonzero(m))\n",
    "        if c < min_count:\n",
    "            continue\n",
    "        center = np.median(ref[m])\n",
    "        vals   = r[m]\n",
    "        if robust:\n",
    "            med = np.median(vals)\n",
    "            sigma = 1.4826 * np.median(np.abs(vals - med))  # MAD→std\n",
    "        else:\n",
    "            sigma = vals.std(ddof=1)\n",
    "        if np.isfinite(sigma) and sigma > 0:\n",
    "            centers.append(center); sigmas.append(sigma); counts.append(c)\n",
    "\n",
    "    centers = np.asarray(centers); sigmas = np.asarray(sigmas); counts = np.asarray(counts)\n",
    "    if centers.size < 2:\n",
    "        raise ValueError(\"有效分箱不足，调小 min_count 或增大 n_bins\")\n",
    "\n",
    "    # 4) 加权最小二乘拟合: sigma ≈ alpha * I + beta\n",
    "    A = np.column_stack([centers, np.ones_like(centers)])\n",
    "    w = np.sqrt(counts)                         # 权重≈置信度\n",
    "    Aw = A * w[:, None]\n",
    "    yw = sigmas * w\n",
    "    (alpha, beta), *_ = np.linalg.lstsq(Aw, yw, rcond=None)\n",
    "\n",
    "    # R^2 诊断\n",
    "    yhat   = A @ np.array([alpha, beta])\n",
    "    ss_res = float(np.sum((sigmas - yhat)**2))\n",
    "    ss_tot = float(np.sum((sigmas - sigmas.mean())**2))\n",
    "    r2     = 1 - ss_res / ss_tot if ss_tot > 0 else np.nan\n",
    "\n",
    "    diag = {\n",
    "        \"R2\": r2,\n",
    "        \"centers\": centers,\n",
    "        \"sigmas\": sigmas,\n",
    "        \"counts\": counts,\n",
    "        \"edges\": edges,\n",
    "        \"used_log_bins\": use_log,\n",
    "    }\n",
    "    return alpha, beta, diag\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def noise_coefficient_estimation(meas_file: str, Vgs_value_num: int, Vds_value_num: int, Vds_range: tuple, Vds_step: float|int, channel_num: int, plot_flag = True, save_flag = True):\n",
    "    '''\n",
    "    Read meas data from .csv and return numpy, \n",
    "    also maps range into Vds in [-3.5, 8.5],\n",
    "    with step size of 0.1,\n",
    "    estimate 3 paramters in order to generate noise on simulated data\n",
    "    '''\n",
    "\n",
    "    # >> read data from measurement file\n",
    "    df = pd.read_csv(meas_file)\n",
    "    df.columns = ['Vgs', 'Vds', 'Ids'] # rename each column\n",
    "\n",
    "\n",
    "    # >> validate dataset\n",
    "    Vgs_unique_value = df['Vgs'].unique()\n",
    "    Vgs_shape = set()\n",
    "    for i in Vgs_unique_value:\n",
    "        Vgs_shape.add(df[df['Vgs'] == i].shape)\n",
    "\n",
    "    print(f'size of original measurement data is {len(Vgs_unique_value)}*{list(list(Vgs_shape)[0])[0]}')\n",
    "\n",
    "    if (len(Vgs_shape) - 1) :\n",
    "        print(f'【Error】sizes of different groups Vgs-bias are not matched with: {Vgs_shape}')\n",
    "    else:\n",
    "        print(f\"sizes of different groups Vgs-bias are matched with: {Vgs_shape}\")\n",
    "\n",
    "    if (len(Vgs_unique_value)) != Vgs_value_num:\n",
    "        raise ValueError(f'The number of different Vgs is given {Vgs_value_num} but is actually {len(Vgs_unique_value)} with Vgs = {Vgs_unique_value.tolist()}')\n",
    "\n",
    "\n",
    "    # >> transform data to numpy and reshape + correct range\n",
    "    # original range : Vds in [-5,20]\n",
    "    #      new range : Vds in [-3.5,8.5]\n",
    "    meas_data_selected = np.zeros((Vgs_value_num, Vds_value_num), dtype=np.float64)\n",
    "    meas_data_smooth = meas_data_selected.copy()\n",
    "    Vgs_unique_value.tolist().sort()\n",
    "    for index, Vgs in enumerate(Vgs_unique_value):\n",
    "        new_Vds_range = np.arange(Vds_range[0], Vds_range[1] + Vds_step, Vds_step)\n",
    "        old_Vds_range = df[df['Vgs'] == Vgs]['Vds'].to_numpy()\n",
    "        old_Ids_range = df[df['Vgs'] == Vgs]['Ids'].to_numpy()\n",
    "        meas_data_selected[index,:] = np.interp(new_Vds_range, old_Vds_range, old_Ids_range)\n",
    "        meas_data_smooth[index,:] = smooth_vec(meas_data_selected[index,:])\n",
    "\n",
    "\n",
    "    # >> params regression - least squares scaling\n",
    "    error_gain, g, alpha, beta = None, None, None, None\n",
    "\n",
    "    # calculate error_gain\n",
    "    error_gain, g = error_gain_calc(meas_data_selected, meas_data_smooth)\n",
    "\n",
    "    # calculate alpha, beta\n",
    "    meas_data_error = meas_data_selected - (g * meas_data_smooth)\n",
    "    std_vector = np.sqrt(np.diag(np.matmul(meas_data_error.T, meas_data_error))/Vgs_value_num)\n",
    "\n",
    "    alpha, beta = np.polyfit(new_Vds_range, std_vector, deg=1)\n",
    "\n",
    "    alpha, beta, dialog = alpha_beta_calc( Id_meas=meas_data_selected,\n",
    "                                        Id_base=meas_data_smooth,\n",
    "                                        g = g ) # TODO: solve this part\n",
    "\n",
    "    # optional, save measurement data with ruled format\n",
    "    if save_flag:\n",
    "        channel_iv = 0\n",
    "        channel_gm = 1\n",
    "        if channel_num == 2:\n",
    "            meas_data_save = np.zeros((channel_num, Vgs_value_num - 1, Vds_value_num), dtype=np.float64)\n",
    "            meas_data_save[channel_iv,:,:] = 0.5*(meas_data_selected[:-1,:] + meas_data_selected[1:,:])\n",
    "            meas_data_save[channel_gm,:,:] = meas_data_selected[1:,:]- meas_data_selected[:-1,:]\n",
    "        elif channel_num == 1:\n",
    "            meas_data_save = np.zeros((Vgs_value_num, Vds_value_num), dtype=np.float64)\n",
    "            meas_data_save[:] = meas_data_selected[:]\n",
    "        else:\n",
    "            raise ValueError(f'channel_num should be 1 or 2, but is {channel_num}')\n",
    "\n",
    "        # check if dir exist\n",
    "        split_index = meas_file[::-1].find('/')\n",
    "        if channel_num == 2:\n",
    "            path = meas_file[:(- split_index)] + 'meas_file_reshape.h5'\n",
    "        elif channel_num == 1:\n",
    "            path = meas_file[:(- split_index)] + 'meas_file_reshape_7row.h5'\n",
    "\n",
    "        with h5py.File(path, 'w') as f:\n",
    "            f.create_dataset('X', data=meas_data_save)\n",
    "            print(f'sucessfully save measurement data under path {path}, with shape of {meas_data_selected.shape}')\n",
    "        \n",
    "\n",
    "    # optional, plot to check performance\n",
    "    if plot_flag:\n",
    "\n",
    "         # test plot 1 - original data vs imterploted data vs smoothed data\n",
    "        _, ax = plt.subplots(2,2, figsize=(18, 13))\n",
    "        for index, Vgs in enumerate(Vgs_unique_value):\n",
    "            range1 = df['Vgs'] == Vgs\n",
    "            range2 = df['Vds'] >= Vds_range[0]\n",
    "            range3 = df['Vds'] <= Vds_range[1]\n",
    "            range = range1 & range2 & range3\n",
    "            # plot only within Vds range [-3.5,8.5]\n",
    "            ax[0,0].plot(df[range]['Vds'], df[range]['Ids'], label = f\"Vgs = {Vgs} V\")\n",
    "            ax[0,1].plot(new_Vds_range, meas_data_selected[index,:], label = f\"Vgs = {Vgs} V\")\n",
    "            ax[1,0].plot(new_Vds_range, smooth_vec(meas_data_selected[index,:]), label = f\"Vgs = {Vgs} V\")\n",
    "            ax[1,1].plot(new_Vds_range, meas_data_selected[index,:] - smooth_vec(meas_data_selected[index,:]), label = f\"Vgs = {Vgs} V\")\n",
    "\n",
    "        ax[0,0].set_title('test plot of original meas data')\n",
    "        ax[0,1].set_title('test plot of interplotted data')\n",
    "        ax[1,0].set_title('test plot of smoothed data')\n",
    "        ax[1,1].set_title('error between interplotted adn sommthed data')\n",
    "\n",
    "        for i, j in np.ndindex((2,2)):\n",
    "            ax[i,j].set_xlabel(\"VDS (V)\")\n",
    "            ax[i,j].set_ylabel(\"IDS (A)\")\n",
    "            ax[i,j].legend()\n",
    "            ax[i,j].grid()\n",
    "        plt.show()\n",
    "\n",
    "        # test plot 2 - regression of alpha and beta\n",
    "        _, ax = plt.subplots(figsize=(9, 6.5))\n",
    "        ax.plot(new_Vds_range, std_vector, 'x', label = 'std of errors')\n",
    "        ax.plot(new_Vds_range, new_Vds_range * alpha + beta,'--', label='regression curve')\n",
    "        ax.set_title('test plot of original meas data')\n",
    "        ax.set_xlabel(\"VDS (V)\")\n",
    "        ax.set_ylabel(\"Standard Deviation of IDS (A)\")\n",
    "        ax.legend()\n",
    "        ax.grid()\n",
    "         \n",
    "    return error_gain, g, alpha, beta\n",
    "\n",
    "\n",
    "# filter rule 1: max value of each sample shoudl be at least > 2, to remove meaningless noise\n",
    "# filter rule 2: with risen Vgs, max Ids must also rise\n",
    "def filter_mask(X: np.ndarray, mask_1_flag = False, mask_2_flag = False) -> np.ndarray:\n",
    "\n",
    "    X = np.asarray(X)\n",
    "    mask_2 = np.array([True]*X.shape[0]).reshape(-1,1)  # default all True\n",
    "    mask_1 = mask_2.copy()\n",
    "    # assert X.ndim == 3 and X.shape[1] == 7 and X.shape[2] == 121\n",
    "    #  -> (N, 7)\n",
    "    last_col = X[:, :, -1]\n",
    "\n",
    "    if mask_1_flag:\n",
    "        max_per_sample = np.max(X, axis=(1,2)).reshape(-1,1)\n",
    "        mask_1 = (max_per_sample > 2)\n",
    "\n",
    "    if mask_2_flag:\n",
    "    # diff in direction of（7） -> (N, 6)，must be strictly increasing\n",
    "        diffs = np.diff(last_col, axis=1)\n",
    "        mask_2 = np.all(diffs > 0, axis=1).reshape(-1,1)\n",
    "\n",
    "    mask = mask_1 & mask_2\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c206c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''STEP 1 : Variable INIT'''\n",
    "\n",
    "# ---------------------------------PATH-------------------------\n",
    "\n",
    "# path of saved .h5 data\n",
    "# generated data path, the data adding noise will be saved in sub folder\n",
    "data_save_path = \"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/data_gen/data/log_bigger_dataset\"\n",
    "noisy_data_path = data_save_path + '/noisy_data'\n",
    "\n",
    "# path of measured data\n",
    "meas_data_path = \"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/data_pre_processing/data\"\n",
    "meas_file_name = \"meas_data.csv\"\n",
    "meas_file = meas_data_path + '/' + meas_file_name\n",
    "\n",
    "\n",
    "# ----------------------------------VAR-------------------------\n",
    "\n",
    "num_set = 100 # here only 10 process/dataset available\n",
    "num_samples_single_set = 1000 # in each process/dataset 100 samples/simulations available\n",
    "# in curret config, input dimension is 2*7*121\n",
    "# channel 1: Ids-Vds curve\n",
    "# channel 2: gm\n",
    "# num_channel = 2  from now on, ignore the channel gm, use only channel Ids-Vds with row=7\n",
    "num_channel = 1\n",
    "channel_iv = 0\n",
    "channel_gm = 1\n",
    "row_X = 7\n",
    "# col_X = 121\n",
    "col_X = 186\n",
    "row_Y = 11\n",
    "col_Y = 1\n",
    "\n",
    "\n",
    "Vgs_value_num = 7\n",
    "# Vds_value_num = 121\n",
    "Vds_value_num = 186\n",
    "# Vds_range = (-3.5,8.5)\n",
    "Vds_range = (-3.5,15)\n",
    "Vds_step = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3d5b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''STEP 2 : calculate Noise Coefficient'''\n",
    "\n",
    "plot_validation_flag = False\n",
    "validation_data_save_flag = True\n",
    "\n",
    "error_gain, g, alpha, beta = noise_coefficient_estimation(meas_file, Vgs_value_num, Vds_value_num, Vds_range, Vds_step, num_channel, plot_validation_flag, validation_data_save_flag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f327d926",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''STEP 3 : add noice in simulation data'''\n",
    "\n",
    "# pre-define matrixs\n",
    "X_iv = np.zeros((num_set*num_samples_single_set, row_X, col_X),dtype=np.float64)\n",
    "X_iv_noisy = X_iv.copy()\n",
    "X_noisy = np.zeros((num_set*num_samples_single_set, num_channel, row_X - 1, col_X),dtype=np.float64) # row minus 1 after size matching\n",
    "Y = np.zeros((num_set*num_samples_single_set, row_Y, col_Y),dtype=np.float64)\n",
    "\n",
    "# trans data from .h5 to numpy\n",
    "for process_id in range(1, num_set + 1):\n",
    "    with h5py.File(f\"{data_save_path}\\\\dataset_process_{process_id}.h5\", 'r') as f:\n",
    "        X_iv[(process_id - 1)*num_samples_single_set : (process_id)*num_samples_single_set, :, :] = f['X_iv']\n",
    "        Y[(process_id - 1)*num_samples_single_set : (process_id)*num_samples_single_set, :, :] = f['Y']\n",
    "\n",
    "\n",
    "# add noise in simulation I-V data\n",
    "sigma_matrix = np.abs(alpha * X_iv + beta)\n",
    "noise = np.random.default_rng().normal(loc=0.0, scale=sigma_matrix)\n",
    "X_iv_noisy = g * X_iv + noise\n",
    "if num_channel == 2:\n",
    "    X_noisy[:, channel_iv, :, :] = 0.5 * (X_iv_noisy[:, :-1,:] + X_iv_noisy[:, 1:,:]) # size match, use not original iv but its interplot\n",
    "    X_noisy[:, channel_gm, :, :] = (X_iv_noisy[:, 1:,:] - X_iv_noisy[:, :-1,:])\n",
    "\n",
    "\n",
    "# remove outliers\n",
    "# filter rule 1: max value of each sample shoudl be at least > 2, to remove meaningless noise\n",
    "# filter rule 2: with risen Vgs, max Ids must also rise\n",
    "rule_1_flag = True\n",
    "rule_2_flag = False\n",
    "outliner_mask = filter_mask(X_iv_noisy, rule_1_flag, rule_2_flag)\n",
    "X_iv_noisy_outlier_removal = X_iv_noisy[outliner_mask.reshape(-1),:,:]\n",
    "Y_outlier_removal = Y[outliner_mask.reshape(-1),:,:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed022ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d45e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''STEP x : Gen 2-stage dataset'''\n",
    "\n",
    "dataset_1 = \"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/data_gen/data/bigger_dataset/noisy_data/train_bigger_dataset_filter_7row.h5\"\n",
    "dataset_2 = \"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/data_gen/data/normal_dataset/noisy_data/train_dataset_7row.h5\"\n",
    "dataset_3 = \"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/data_gen/data/log_normal_dataset/noisy_data/train_log_dataset_7row_2.h5\"\n",
    "dataset_4 = \"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/data_gen/data/log_normal_dataset/noisy_data/train_log_dataset_7row_1.h5\"\n",
    "\n",
    "\n",
    "f1 = h5py.File(dataset_1, 'r')\n",
    "f2 = h5py.File(dataset_2, 'r')\n",
    "f3 = h5py.File(dataset_3, 'r')\n",
    "f4 = h5py.File(dataset_4, 'r')\n",
    "\n",
    "print('f1-x shape: ',f1['X'].shape)\n",
    "print('f2-x shape: ',f2['X'].shape)\n",
    "print('f3-x shape: ',f3['X'].shape)\n",
    "print('f4-x shape: ',f4['X'].shape)\n",
    "print('f-x type: ',type(f3['X']))\n",
    "\n",
    "f_X = np.vstack((f1['X'],f2['X'],f3['X']))\n",
    "f_Y = np.vstack((f1['Y'],f2['Y'],f3['Y']))\n",
    "\n",
    "meas_like_X = f4['X']\n",
    "meas_like_Y = f4['Y']\n",
    "\n",
    "print('f_X shape: ',f_X.shape)\n",
    "print('f_Y shape: ',f_Y.shape)\n",
    "\n",
    "print('meas_like_X shape: ',meas_like_X.shape)\n",
    "print('meas_like_Y shape: ',meas_like_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269cc0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_1_flag = True\n",
    "rule_2_flag = True\n",
    "outliner_mask = filter_mask(f_X, rule_1_flag, rule_2_flag)\n",
    "meas_like_outliner_mask = filter_mask(meas_like_X, rule_1_flag, rule_2_flag)\n",
    "\n",
    "train_X = f_X[outliner_mask.reshape(-1),:,:]\n",
    "train_Y = f_Y[outliner_mask.reshape(-1),:,:]\n",
    "\n",
    "meas_like_X = meas_like_X[meas_like_outliner_mask.reshape(-1),:,:]\n",
    "meas_like_Y = meas_like_Y[meas_like_outliner_mask.reshape(-1),:,:]\n",
    "\n",
    "print('shape of full training set X: ',train_X.shape)\n",
    "print('shape of full training set Y: ',train_Y.shape)\n",
    "\n",
    "print('shape of meas like fine-turning set X: ',meas_like_X.shape)\n",
    "print('shape of meas like fine-turning set Y: ',meas_like_Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a799988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_save_path = \"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/NN_training/dataset/training/stage_1_full_data.h5\"\n",
    "meas_like_data_save_path = \"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/NN_training/dataset/training/stage_2_ml_data.h5\"\n",
    "full_plus_ml_data_save_path = \"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/NN_training/dataset/training/stage_1_fpml_data.h5\"\n",
    "\n",
    "with h5py.File(full_data_save_path, 'w') as f:\n",
    "    f.create_dataset('X', data=train_X)\n",
    "    f.create_dataset('Y', data=train_Y)\n",
    "\n",
    "with h5py.File(meas_like_data_save_path, 'w') as f:\n",
    "    f.create_dataset('X', data=meas_like_X)\n",
    "    f.create_dataset('Y', data=meas_like_Y)\n",
    "\n",
    "with h5py.File(full_plus_ml_data_save_path, 'w') as f:\n",
    "    f.create_dataset('X', data=np.vstack((train_X, meas_like_X)))\n",
    "    f.create_dataset('Y', data=np.vstack((train_Y, meas_like_Y)))\n",
    "\n",
    "f1.close()\n",
    "f2.close()\n",
    "f3.close()\n",
    "f4.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7865197",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_iv_noisy.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "\n",
    "print(X_iv_noisy.shape, X_iv_noisy_outlier_removal.shape)\n",
    "print(Y.shape, Y_outlier_removal.shape)\n",
    "# print(np.where(outliner_mask==False)[0])\n",
    "print(np.where(outliner_mask==False)[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aebcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''STEP 4 : save noisy data as torch'''\n",
    "\n",
    "# check if dir exist\n",
    "if not os.path.exists(noisy_data_path):\n",
    "    os.makedirs(noisy_data_path)\n",
    "    print('dir not found, create new dir based on given path')\n",
    "else:\n",
    "    ('path exists')\n",
    "print('saving data ...')\n",
    "\n",
    "if num_channel == 2:\n",
    "    file_name = 'train_dataset_6row.h5'\n",
    "    path = noisy_data_path + '/' + file_name\n",
    "    with h5py.File(path, 'w') as f:\n",
    "        f.create_dataset('X', data=X_noisy)\n",
    "        f.create_dataset('Y', data=Y)\n",
    "elif num_channel == 1:\n",
    "    file_name = 'train_bigger_dataset_7row.h5'\n",
    "    path = noisy_data_path + '/' + file_name\n",
    "    with h5py.File(path, 'w') as f:\n",
    "        f.create_dataset('X', data=X_iv_noisy_outlier_removal)\n",
    "        f.create_dataset('Y', data=Y_outlier_removal)\n",
    "        # f.create_dataset('X', data=X_iv_noisy)\n",
    "        # f.create_dataset('Y', data=Y)\n",
    "\n",
    "\n",
    "print('data saved !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542037cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_channel == 1:\n",
    "    file_name = 'train_bigger_dataset_7row.h5'\n",
    "    path = noisy_data_path + '/' + file_name\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        print(f['X'].shape)\n",
    "    \n",
    "    file_name = 'train_bigger_dataset_filter_7row.h5'\n",
    "    path = noisy_data_path + '/' + file_name\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        print(f['X'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ded2720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test :: build a classifier to findout the data matching the distribution of meas data\n",
    "# remove outliers\n",
    "max_per_sample = np.max(X_iv_noisy_outlier_removal, axis=(1,2)).reshape(-1,1)\n",
    "ratio_vector = np.zeros((max_per_sample.shape[0],1),dtype=np.float64)\n",
    "for index in range(max_per_sample.shape[0]):\n",
    "    ratio_vector[index] = np.mean(np.diff(X_iv_noisy_outlier_removal[index, :, -1],axis = 0)) / (max_per_sample[index, 0] / 6)\n",
    "\n",
    "positive_ratio_index = ratio_vector > 0\n",
    "\n",
    "print(f'ratio is {ratio_vector}')\n",
    "print(f'positive ratio index is {np.where(positive_ratio_index==True)[0]}', f'with size of {np.where(positive_ratio_index==True)[0].shape}')\n",
    "print(np.max(ratio_vector), np.where(ratio_vector==np.max(ratio_vector))[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7754b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test :: merge multiple different .h5 datasets\n",
    "\n",
    "file_name_1 = 'E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/data_gen/data/log_bigger_dataset/noisy_data/train_bigger_dataset_7row_186col.h5'\n",
    "\n",
    "file_name_2 = 'E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/data_gen/data/log_normal_dataset/noisy_data/train_log_dataset_7row_double.h5'\n",
    "\n",
    "save_path = \"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/NN_training/dataset/training\"\n",
    "\n",
    "merge_file_name = 'train_log_full_bigger_dataset_7row.h5'\n",
    "\n",
    "f1 = h5py.File(file_name_1, 'r')\n",
    "f2 = h5py.File(file_name_2, 'r')\n",
    "\n",
    "# match size\n",
    "f_iterator = [f1,f2]\n",
    "f_col = set(i['X'].shape[2] for i in f_iterator)\n",
    "for index,f in enumerate(f_iterator):\n",
    "    print(f\"before correction: f{index+1}['X'].shape = {f['X'].shape}, f{index+1}['Y'].shape = {f['Y'].shape}\")\n",
    "\n",
    "# merge data\n",
    "match_col = min(f_col)\n",
    "if len(f_iterator) > 2:\n",
    "    print(f'[Diag] col-num not matched, unified to {match_col}')\n",
    "    data_x = np.vstack((f_iterator[0]['X'][:,:,:match_col], f_iterator[1]['X'][:,:,:match_col]))\n",
    "    data_y = np.vstack((f_iterator[0]['Y'], f_iterator[1]['Y']))\n",
    "    for f in f_iterator[2:]:\n",
    "        data_x = np.vstack((data_x, f['X'][:,:,:match_col]))\n",
    "        data_y = np.vstack((data_y, f['Y']))\n",
    "else:\n",
    "    data_x = np.vstack((f_iterator[0]['X'][:,:,:match_col], f_iterator[1]['X'][:,:,:match_col]))\n",
    "    data_y = np.vstack((f_iterator[0]['Y'], f_iterator[1]['Y']))\n",
    "\n",
    "print(f\"after correction: data['X'].shape = {data_x.shape}, data['Y'].shape = {data_y.shape}\")\n",
    "\n",
    "# save data\n",
    "path = save_path + '/' + merge_file_name\n",
    "with h5py.File(path, 'w') as f:\n",
    "    f.create_dataset('X', data=data_x)\n",
    "    f.create_dataset('Y', data=data_y)\n",
    "\n",
    "print('data saved !')\n",
    "for f in f_iterator:\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7fac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "meas_path = \"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/data_pre_processing/data/meas_file_reshape_7row.h5\"\n",
    "save_path = \"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/data_pre_processing/data/temp.h5\"\n",
    "\n",
    "f = h5py.File(meas_path, 'r')\n",
    "print(f.keys())\n",
    "print(f['X'])\n",
    "\n",
    "temp = np.array(f['X'][:]).reshape(1,7,121)\n",
    "print(temp.shape)\n",
    "f.close()\n",
    "\n",
    "with h5py.File(save_path, 'w') as f:\n",
    "    f.create_dataset('X', data=temp)\n",
    "    print(f.keys())\n",
    "    print(f['X'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023be0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test plot\n",
    "\n",
    "index = 4000\n",
    "index -= 1\n",
    "\n",
    "legend_values = np.linspace(-3.5, 8.6, X_noisy.shape[3])\n",
    "  \n",
    "# create colormap\n",
    "cmap = plt.cm.gist_ncar\n",
    "norm = plt.Normalize(vmin=legend_values.min(), vmax=legend_values.max())\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 8)) \n",
    "for i in range(X_noisy.shape[2]):\n",
    "    # plot 1\n",
    "    color = cmap(norm(legend_values[i]))\n",
    "    ax[0].plot(np.arange(-3.5, 8.6, 0.1), X_noisy[index, channel_iv, i, :])\n",
    "    ax[0].grid(True)\n",
    "    ax[0].set_title('noisy I-V')\n",
    "    ax[0].set_xlabel(\"Vds (V)\")\n",
    "    ax[0].set_ylabel(\"Ids\")\n",
    "\n",
    "for i in range(X_noisy.shape[3]):\n",
    "    # plot 2\n",
    "    color = cmap(norm(legend_values[i]))\n",
    "    ax[1].plot(np.arange(1.5, 7.5, 1), X_noisy[index, channel_iv, :, i], color=color)\n",
    "    ax[1].grid(True)\n",
    "    ax[1].set_title('noisy I-V')\n",
    "    ax[1].set_xlabel(\"Vgs (V)\")\n",
    "    ax[1].set_ylabel(\"Ids\")\n",
    "    # plot 3\n",
    "    color = cmap(norm(legend_values[i]))\n",
    "    ax[2].plot(np.arange(1.5, 7.5, 1), X_noisy[index, channel_gm, :, i], color=color)\n",
    "    ax[2].grid(True)\n",
    "    ax[2].set_title('noisy gm')\n",
    "    ax[2].set_xlabel(\"Vgs (V)\")\n",
    "    ax[2].set_ylabel(\"Ids\")\n",
    "\n",
    "# 加颜色条\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])  # 仅用于colorbar\n",
    "cbar = fig.colorbar(sm, ax=ax)\n",
    "cbar.set_label(\"Vds (V)\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
