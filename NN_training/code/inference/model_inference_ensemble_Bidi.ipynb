{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8368f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is part of the ADS Parameter Fitting project.\n",
    "# must be used under ADS integrated Python env (A)\n",
    "# namely, ..\\ADS_install_path\\tools\\python\\python.exe --> Python 3.13.2\n",
    "# TODO: whole script is run in Jupyter because ADS python ADI only\n",
    "# supports IPython kernel !!!\n",
    "\n",
    "# packages to build DIR env\n",
    "import os, json\n",
    "# set ads dict: HPEESOF_DIR and home director : HOME\n",
    "os.environ['HPEESOF_DIR'] = 'D:/ADS/install'\n",
    "os.environ['HOME'] = 'D:/ADS/dir'\n",
    "\n",
    "# # packages to multiprocessing\n",
    "# # add current working directory\n",
    "# import sys\n",
    "# cur_path_nn = \"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting_local/IV_param_regression/NN_training\"\n",
    "# sys.path.append(cur_path_nn)\n",
    "\n",
    "# packages to import ADS\n",
    "from keysight.ads import de\n",
    "from keysight.ads.de import db_uu as db\n",
    "from keysight.edatoolbox import ads\n",
    "import keysight.ads.dataset as dataset\n",
    "from keysight.edatoolbox import util\n",
    "from pathlib import Path\n",
    "from IPython.core import getipython\n",
    "\n",
    "# packages for model inference\n",
    "from infer_sup_packg import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"Conversion of an array with ndim > 0 to a scalar is deprecated\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b79131f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary class and functions for ADS simulation\n",
    "\n",
    "class PyADS():\n",
    "    def __init__(self):\n",
    "        self.HPEESOF_DIR = 'D:/ADS/install'\n",
    "        self.HOME = 'D:/ADS/dir'\n",
    "        self.cur_workspace_path = None\n",
    "        self.workspace = None\n",
    "        self.cur_library_name = None\n",
    "        self.library = None\n",
    "        self.cur_design_name = None\n",
    "        self.design = None\n",
    "\n",
    "    def create_and_open_an_empty_workspace(self, workspace_path: str):\n",
    "    # example : workspace_path = \"C:/ADS_Python_Tutorials/tutorial1_wrk\"\n",
    "    # Ensure there isn't already a workspace open\n",
    "        if de.workspace_is_open():\n",
    "            de.close_workspace()\n",
    "    \n",
    "        # Cannot create a workspace if the directory already exists\n",
    "        if os.path.exists(workspace_path):\n",
    "            raise RuntimeError(f\"Workspace directory already exists: {workspace_path}\")\n",
    "    \n",
    "        # Create the workspace\n",
    "        workspace = de.create_workspace(workspace_path)\n",
    "        # Open the workspace\n",
    "        workspace.open()\n",
    "        # Return the open workspace and close when it finished\n",
    "        return workspace\n",
    "    \n",
    "    def create_a_library_and_add_it_to_the_workspace(self, workspace: de.Workspace, library_name: str) -> None:\n",
    "        #assert workspace.path is not None\n",
    "        # Libraries can only be added to an open workspace\n",
    "        assert workspace.is_open\n",
    "        # We'll create a library in the directory of the workspace\n",
    "        library_path = workspace.path / library_name\n",
    "        # Create the library\n",
    "        de.create_new_library(library_name, library_path)\n",
    "        # And add it to the workspace (update lib.defs)\n",
    "        workspace.add_library(library_name, library_path, de.LibraryMode.SHARED)\n",
    "        lib=workspace.open_library(library_name,library_path,de.LibraryMode.SHARED)\n",
    "        return lib\n",
    "\n",
    "    # @timeout_thread_soft(20) # 20s timeout\n",
    "    def schematic_simulation(self, workspace_path: str, library_name: str, design_name: str, instance_name: str, var_dict: dict, vgs_bias_param_sweep_name: str, vds_bias_param_sweep_name: str, vgs_bias_simulation_name: str, vds_bias_simulation_name: str, verilog_a_dir:str = None) -> None:\n",
    "        ''' Load Path and files, Edit the design variables, Simulate the design, and return the dataset '''\n",
    "\n",
    "        # test timeout\n",
    "        # random_number = random.random()\n",
    "        # time.sleep(13) if random_number >= 0.5 else time.sleep(0.5)\n",
    "\n",
    "        # >> Load Path and files\n",
    "        if not os.path.exists(workspace_path):\n",
    "            raise RuntimeError(f\"Workspace directory doesn't exist: {workspace_path}\")\n",
    "        if de.workspace_is_open():\n",
    "            de.close_workspace()\n",
    "        \n",
    "        # Open the workspace\n",
    "        # if (not self.workspace) or (self.cur_workspace_path != workspace_path):\n",
    "        self.workspace = de.open_workspace(workspace_path)\n",
    "        self.cur_workspace_path = workspace_path\n",
    "        # Open the library\n",
    "        # if (not self.library) or (self.cur_library_name != library_name):\n",
    "        self.library = self.workspace.open_library(lib_name=library_name, mode=de.LibraryMode.SHARED)\n",
    "        self.cur_library_name = library_name\n",
    "        # Open the design\n",
    "        # if (not self.design) or (self.cur_design_name != design_name):\n",
    "        self.design = db.open_design((library_name, design_name, \"schematic\"), db.DesignMode.APPEND)\n",
    "        self.cur_design_name = design_name\n",
    "\n",
    "        # >> Edit the design variables\n",
    "        # edit VAR\n",
    "        v = self.design.get_instance(inst_name=instance_name)\n",
    "        assert v.is_var_instance\n",
    "        for var_name in var_dict:\n",
    "            v.vars[var_name] = var_dict[var_name]\n",
    "        # Save the design\n",
    "        self.design.save_design()\n",
    "        # Simulate the design\n",
    "        output_dir = os.path.join(self.workspace.path, \"output\")\n",
    "        netlist_file = os.path.join(output_dir, \"data_gen.ckt\")\n",
    "        output_file =  os.path.join(output_dir, \"data_gen.ckt.out\")\n",
    "        # create the simulation output directory\n",
    "        util.safe_makedirs(output_dir)\n",
    "\n",
    "        # >> Simulate and return the dataset\n",
    "        # ipython = getipython.get_ipython()\n",
    "        # if ipython is None:\n",
    "        #     print(\"The remaining portion of the script must be run in an IPython environment. Exiting.\")\n",
    "        #     return\n",
    "        # capture the netlist in a string\n",
    "        netlist = self.design.generate_netlist()\n",
    "        # access to the simulator object to run netlists\n",
    "        simulator = ads.CircuitSimulator()\n",
    "        # run the netlist, this will block output\n",
    "        simulator.run_netlist(netlist, output_dir=output_dir, netlist_file=netlist_file, output_file=output_file, verilog_dir=verilog_a_dir)\n",
    "        output_data = dataset.open(Path(os.path.join(output_dir, f\"{design_name}.ds\")))\n",
    "        \n",
    "        # >> return data in pandas DataFrame format\n",
    "        # <class 'pandas.core.frame.DataFrame'>\n",
    "        data_ids_vds = output_data[f'{vgs_bias_param_sweep_name}.{vgs_bias_simulation_name}.DC'].to_dataframe().reset_index()\n",
    "        data_ids_vgs = output_data[f'{vds_bias_param_sweep_name}.{vds_bias_simulation_name}.DC'].to_dataframe().reset_index()\n",
    "        return data_ids_vds, data_ids_vgs\n",
    "    \n",
    "\n",
    "\n",
    "    def dataset_reshape(self, pd_data_IV: pd.DataFrame, pd_data_gm: pd.DataFrame, IV_dimension: list, gm_dimension: list, var_dict: dict):\n",
    "        ''' reshape the dataset into desired input matrix and output vector '''\n",
    "        IV_row_count = IV_dimension[0] # Vgs\n",
    "        IV_col_count = IV_dimension[1] # Vds\n",
    "        gm_row_count = gm_dimension[0] # Vds\n",
    "        gm_col_count = gm_dimension[1] # Vgs\n",
    "\n",
    "        output_x_IV = np.empty((IV_row_count, IV_col_count),dtype=np.float64)\n",
    "        output_x_gm = np.empty((gm_row_count, gm_col_count),dtype=np.float64)\n",
    "        output_y = np.empty((len(var_dict), 1),dtype=np.float64)\n",
    "\n",
    "        IV_row = pd_data_IV[\"VGS\"].drop_duplicates().sort_values(ascending=True).tolist() # attention here: must be descending order\n",
    "        gm_row = pd_data_gm[\"VDS\"].drop_duplicates().sort_values(ascending=True).tolist()\n",
    "\n",
    "        for index,row_value in enumerate(IV_row):\n",
    "            output_x_IV[index, :] = pd_data_IV.loc[pd_data_IV['VGS'] == row_value, 'IDS.i'].to_numpy()\n",
    "        for index,row_value in enumerate(gm_row):\n",
    "            output_x_gm[index, :] = pd_data_gm.loc[pd_data_gm['VDS'] == row_value, 'IDS.i'].to_numpy()\n",
    "        for index, item in enumerate(var_dict):\n",
    "            output_y[index, 0] = var_dict[item]\n",
    "\n",
    "        return output_x_IV, output_x_gm, output_y\n",
    "    \n",
    "\n",
    "def param_random_generator(param_range: dict):\n",
    "    ''' generate a random parameter set for the HEMT model '''\n",
    "    # define the parameter range\n",
    "    # param_range = {\n",
    "    #     'VOFF': (-1.2, 2.6),\n",
    "    #     'U0': (0, 2.2),\n",
    "    #     'NS0ACCS': (1e15, 1e20),\n",
    "    #     'NFACTOR': (0.1, 5),\n",
    "    #     'ETA0': (0, 1),\n",
    "    #     'VSAT': (5e4, 1e7),\n",
    "    #     'VDSCALE': (0.5, 1e6),\n",
    "    #     'CDSCD': (1e-5, 0.75),\n",
    "    #     'LAMBDA': (0, 0.2),\n",
    "    #     'MEXPACCD': (0.05, 12),\n",
    "    #     'DELTA': (2, 100)\n",
    "    # }\n",
    "    # generate random parameters\n",
    "    var_dict = {key: str(np.random.uniform(low=val[0], high=val[1])) for key, val in param_range.items()}\n",
    "    return var_dict\n",
    "\n",
    "def init_h5_file(h5_path, x_iv_shape, x_gm_shape, y_shape,\n",
    "                 dtype_x=np.float64, dtype_y=np.float64):\n",
    "    with h5py.File(h5_path, 'w') as f:\n",
    "        # X: [num_samples, m, n]\n",
    "        f.create_dataset(\n",
    "            'X_iv',\n",
    "            shape=(0, x_iv_shape[0], x_iv_shape[1]),\n",
    "            maxshape=(None, x_iv_shape[0], x_iv_shape[1]),\n",
    "            dtype=dtype_x\n",
    "        )\n",
    "        f.create_dataset(\n",
    "            'X_gm',\n",
    "            shape=(0, x_gm_shape[0], x_gm_shape[1]),\n",
    "            maxshape=(None, x_gm_shape[0], x_gm_shape[1]),\n",
    "            dtype=dtype_x\n",
    "        )\n",
    "        # Y: [num_samples, y_len]\n",
    "        f.create_dataset(\n",
    "            'Y',\n",
    "            shape=(0, y_shape[0], 1),\n",
    "            maxshape=(None, y_shape[0], 1),\n",
    "            dtype=dtype_y\n",
    "        )\n",
    "\n",
    "def append_to_h5(h5_path, x_iv_new, x_gm_new, y_new):\n",
    "    x_iv_new = np.asarray(x_iv_new, dtype=np.float64)\n",
    "    x_gm_new = np.asarray(x_gm_new, dtype=np.float64)\n",
    "    y_new = np.asarray(y_new, dtype=np.float64)\n",
    "\n",
    "    # 确保 y_new 是二维 (batch_size, y_len)\n",
    "    if y_new.ndim == 2:\n",
    "        y_new = y_new.reshape(-1, 1)\n",
    "    else:\n",
    "        raise RuntimeError(f\"y_new must be vector, but got shape {y_new.shape}\")\n",
    "\n",
    "    with h5py.File(h5_path, 'a') as f:\n",
    "        ds_x_iv = f['X_iv']\n",
    "        ds_x_gm = f['X_gm']\n",
    "        ds_y = f['Y']\n",
    "\n",
    "        cur_len = ds_x_iv.shape[0]\n",
    "        new_len = cur_len + 1\n",
    "\n",
    "        # 扩展\n",
    "        ds_x_iv.resize(new_len, axis=0)\n",
    "        ds_x_gm.resize(new_len, axis=0)\n",
    "        ds_y.resize(new_len, axis=0)\n",
    "\n",
    "        # 赋值\n",
    "        ds_x_iv[cur_len:new_len, :, :] = x_iv_new\n",
    "        ds_x_gm[cur_len:new_len, :, :] = x_gm_new\n",
    "        ds_y[cur_len:new_len, :, :] = y_new\n",
    "\n",
    "\n",
    "def singel_process_iteration_data_gen2h5(workspace_path: str, \n",
    "                                         validate_dict: dict, \n",
    "                                         library_name: str, \n",
    "                                         design_name: str, \n",
    "                                         instance_name: str, \n",
    "                                         param_range: dict, \n",
    "                                         vgs_bias_param_sweep_name: str, \n",
    "                                         vds_bias_param_sweep_name: str, \n",
    "                                         vgs_bias_simulation_name: str, \n",
    "                                         vds_bias_simulation_name: str, \n",
    "                                         data_shape: list, \n",
    "                                         iteration_num: int, \n",
    "                                         process_id: int, \n",
    "                                         save_path: str, \n",
    "                                         verilog_a_dir: str = None, \n",
    "                                         new_start: bool = True, \n",
    "                                         old_stop_index: int = 0, \n",
    "                                         dtype_x=np.float64, dtype_y=np.float64):\n",
    "    ''' generate dataset in single process iteration '''\n",
    "    # create an instance of the PyADS class\n",
    "    ads_ctrl = PyADS()\n",
    "    X_iv_shape = data_shape[0]\n",
    "    X_gm_shape = data_shape[1]\n",
    "    Y_shape = data_shape[2]\n",
    "    num_sampling = len(validate_dict['VSAT']) if type(validate_dict['VSAT'])!=str else 1\n",
    "    iteration_num = num_sampling\n",
    "\n",
    "    # init h5 file\n",
    "    if new_start:\n",
    "        init_h5_file(f\"{save_path}\\\\validate.h5\", X_iv_shape, X_gm_shape, Y_shape)\n",
    "    # on cvae + rand mode, transform a big dict into several small dict available for iterator\n",
    "    validate_dict_list = None\n",
    "    if num_sampling != 1:\n",
    "        validate_dict_list = [dict(zip(validate_dict.keys(), values)) for values in zip(*validate_dict.values())]\n",
    "\n",
    "    for i in range(num_sampling) if new_start else range(old_stop_index, iteration_num):\n",
    "        while True:\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                if validate_dict:\n",
    "                    var_dict = validate_dict if not validate_dict_list else validate_dict_list[i]\n",
    "                else:\n",
    "                # var_dict = param_random_generator(param_range)\n",
    "                    var_dict = param_random_generator_log(param_range) # use log-uniform distribution based generator\n",
    "                pd_data_vgs_bias, pd_data_gm = ads_ctrl.schematic_simulation(\n",
    "                    workspace_path,\n",
    "                    library_name,\n",
    "                    design_name,\n",
    "                    instance_name,\n",
    "                    var_dict,\n",
    "                    vgs_bias_param_sweep_name,\n",
    "                    vds_bias_param_sweep_name,\n",
    "                    vgs_bias_simulation_name,\n",
    "                    vds_bias_simulation_name,\n",
    "                    verilog_a_dir\n",
    "                )\n",
    "                X_iv, X_gm, y = ads_ctrl.dataset_reshape(pd_data_vgs_bias, pd_data_gm, X_iv_shape, X_gm_shape, var_dict)\n",
    "                end_time = time.time()\n",
    "                print(f\" >> Process {process_id} :: Loop {i+1}/{iteration_num} :: used time: {round(end_time - start_time, 2)} s\")\n",
    "                try:\n",
    "                    if not validate_dict:\n",
    "                        append_to_h5(f\"{save_path}\\\\dataset_process_{process_id}.h5\", X_iv, X_gm, y)\n",
    "                    else:\n",
    "                        append_to_h5(f\"{save_path}\\\\validate.h5\", X_iv, X_gm, y)\n",
    "                except:\n",
    "                    print(f\"【ERROR】Error appending data in process {process_id} at iteration {i + 1}\")\n",
    "                break\n",
    "\n",
    "            # except TimeoutError:\n",
    "            #     end_time = time.time()\n",
    "            #     print(f' >> Process {process_id} :: Loop {i + 1}/{iteration_num} [Failed because Timeout] :: used time:', round(end_time - start_time, 2), 's', file=sys.stderr, flush=True)\n",
    "            #     time.sleep(3)\n",
    "            #     ads_ctrl.close()\n",
    "            #     time.sleep(10)\n",
    "            #     continue\n",
    "            except:\n",
    "                end_time = time.time()\n",
    "                print(f' >> Process {process_id} :: Loop {i + 1}/{iteration_num} [Failed because cannot converge] :: used time:', round(end_time - start_time, 2), 's')\n",
    "                time.sleep(1)\n",
    "                # ads_ctrl.close()\n",
    "                # time.sleep(2)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eea90cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ADS\n",
    "# DEFINE VARIABLES\n",
    "\n",
    "workspace_path = \"E:\\\\personal_Data\\\\Document of School\\\\Uni Stuttgart\\\\Masterarbeit\\\\Simulation\\\\ADS\\\\GaN4EMoBiL_BiDi_GaN_wrk\"\n",
    "verilog_a_dir = workspace_path + \"\\\\veriloga\"\n",
    "validate_dict = None\n",
    "library_name = \"GaN4EMoBiL_BiDi_GaN_lib\"\n",
    "design_name = \"Optimizing_ADS_Bidi_Chenyan\"\n",
    "instance_name = \"sweep_param\"\n",
    "var_dict_default = {'VOFF':'-4.1', 'U0':'0.4', 'NS0ACCS':'5e+16', 'NFACTOR':'0', 'ETA0':'1e-9', 'VSAT':'1.9e5', 'VDSCALE':'5', 'CDSCD':'0.05', 'LAMBDA':'0.01', 'MEXPACCD':'2', 'DELTA':'2'}\n",
    "param_range = {\n",
    "    'VOFF': (-4.7, -4),\n",
    "    'U0': (0.1, 2.2),\n",
    "    'NS0ACCS': (1e15, 1e20),\n",
    "    'NFACTOR': (0, 10),\n",
    "    'ETA0': (0, 1),\n",
    "    'VSAT': (5e4, 1e7),\n",
    "    'VDSCALE': (0.5, 1e6),\n",
    "    'CDSCD': (0, 0.75),\n",
    "    'LAMBDA': (0, 0.2),\n",
    "    'MEXPACCD': (0, 12),\n",
    "    'DELTA': (2, 100)\n",
    "    }\n",
    "\n",
    "X_iv_shape = [9,61]\n",
    "X_gm_shape = [6,101]\n",
    "Y_shape = [11,1]\n",
    "X_goal_dict = {\n",
    "    'goal': \"X_gm\",\n",
    "    'row_range': (1, 6),\n",
    "    'col_range': (-10, 0),\n",
    "    'row_num': X_gm_shape[0],\n",
    "    'col_num': X_gm_shape[1]\n",
    "}\n",
    "data_shape = [X_iv_shape, X_gm_shape, Y_shape]\n",
    "vgs_bias_param_sweep_name = 'Sweep_vgs'\n",
    "vds_bias_param_sweep_name = 'Sweep_vds'\n",
    "vgs_bias_simulation_name = 'Output'\n",
    "vds_bias_simulation_name = 'Transfer'\n",
    "iteration_num = 1000\n",
    "process_id = 1\n",
    "data_save_path = \"E:\\\\personal_Data\\\\Document of School\\\\Uni Stuttgart\\\\Masterarbeit\\\\Code\\\\param_regression\\\\ADS_Parameter_Fitting\\\\IV_param_regression\\\\NN_training\\\\dataset\\\\temp_generated\"\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# For NN model inference\n",
    "\n",
    "python_path = r\"D:/Miniconda/envs/DL/python.exe\"  # model based python path\n",
    "script_dir  = r\"E:\\\\personal_Data\\\\Document of School\\\\Uni Stuttgart\\\\Masterarbeit\\\\Code\\\\param_regression\\\\ADS_Parameter_Fitting\\\\IV_param_regression\\\\NN_training\\\\code\\\\training\" # path of code\n",
    "code_file_name = \"asm_hemt_cvae_Bidi.py\"\n",
    "training_data_path   = r\"E:\\\\personal_Data\\\\Document of School\\\\Uni Stuttgart\\\\Masterarbeit\\\\Code\\\\param_regression\\\\ADS_Parameter_Fitting\\\\IV_param_regression\\\\NN_training\\\\dataset\\\\training\\\\Bidi_Transfer_log_merge.h5\"  # path of training dataset\n",
    "\n",
    "# meas_like_val_set = r\"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/NN_training/dataset/training/stage_2_ml_data.h5\" \n",
    "\n",
    "meas_path = r\"E:\\\\personal_Data\\\\Document of School\\\\Uni Stuttgart\\\\Masterarbeit\\\\Code\\\\param_regression\\\\ADS_Parameter_Fitting\\\\IV_param_regression\\\\NN_training\\\\dataset\\\\training\\\\meas_smoothed_Bidi_Transfer.h5\"\n",
    "\n",
    "output_csv_path = r\"E:\\\\personal_Data\\\\Document of School\\\\Uni Stuttgart\\\\Masterarbeit\\\\Code\\\\param_regression\\\\ADS_Parameter_Fitting\\\\IV_param_regression\\\\NN_training\\\\dataset\\\\temp_generated\\\\pred_7row.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed6150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''single model inference'''\n",
    "\n",
    "model_path = r\"E:\\\\personal_Data\\\\Document of School\\\\Uni Stuttgart\\\\Masterarbeit\\\\Code\\\\param_regression\\\\ADS_Parameter_Fitting\\\\IV_param_regression\\\\NN_training\\\\model\\\\cvae_Bidi_1_Channel\\\\version_3_8\"\n",
    "\n",
    "# basic params\n",
    "inference_data_path = meas_path # path of inference dataset\n",
    "# inference_data_path = training_data_path # path of inference dataset\n",
    "inference_data_index = \"None\" #  from 0 to 9999\n",
    "# \"None\"    : for 1. single input with shape (2,6,121)\n",
    "#                 2. multile input with shaple (N,2,6,121)\n",
    "# str(int)  : for single input with shape (N,2,6,121)\n",
    "single_input_mode = bool(True)\n",
    "cvae_ena = True\n",
    "cvae_mode = 'rand'\n",
    "num_sampling = 300 # only used when cvae_mode = 'rand'\n",
    "dropout_enale = False\n",
    "\n",
    "\n",
    "# Model inference\n",
    "print(\" >> Start model inference...\")\n",
    "# var_dict = model_inference(\n",
    "#     python_path=python_path,\n",
    "#     script_dir=script_dir,\n",
    "#     code_file_name=\"asm_hemt_2stage_dnn.py\" if not cvae_ena else \"asm_hemt_cvae.py\",\n",
    "#     model_path=model_path,\n",
    "#     inference_data_path=inference_data_path,\n",
    "#     inference_data_index=inference_data_index,\n",
    "#     output_csv_path=output_csv_path,\n",
    "#     single_input_mode=single_input_mode,\n",
    "#     cvae_ena=cvae_ena,\n",
    "#     cvae_mode=cvae_mode,\n",
    "#     num_sampling=num_sampling\n",
    "# )\n",
    "var_dict = model_inference(\n",
    "    python_path=python_path,\n",
    "    script_dir=script_dir,\n",
    "    code_file_name=code_file_name,\n",
    "    model_path=model_path,\n",
    "    inference_data_path=inference_data_path,\n",
    "    inference_data_index=inference_data_index,\n",
    "    output_csv_path=output_csv_path,\n",
    "    single_input_mode=single_input_mode,\n",
    "    cvae_ena=cvae_ena,\n",
    "    cvae_mode=cvae_mode,\n",
    "    num_sampling=num_sampling,\n",
    "    dropout_enable=dropout_enale,\n",
    ")\n",
    "\n",
    "# range correction\n",
    "print(\" >> Start output range correction...\")\n",
    "row_def = num_sampling if cvae_ena and cvae_mode=='rand' else 1\n",
    "val_matrix = np.full((row_def, len(param_range)), 'OK', dtype=\"<U10\")\n",
    "var_matrix = np.full((row_def, len(param_range)), 0.0, dtype=np.float64)\n",
    "for index,key in enumerate(param_range):\n",
    "    var_matrix[:,index] = np.array(var_dict[key], dtype=float)\n",
    "for col,key in enumerate(param_range):\n",
    "    (low,high) = param_range[key]\n",
    "    low_mask = var_matrix[:,col] < low\n",
    "    high_mask = var_matrix[:,col] > high\n",
    "    val_matrix[low_mask,col] = 'Underflow'\n",
    "    var_matrix[low_mask,col] = low\n",
    "    val_matrix[high_mask,col] = 'Overflow'\n",
    "    var_matrix[high_mask,col] = high\n",
    "    var_dict[key] = var_matrix[:,col].astype(str)\n",
    "if ('Underflow' in val_matrix) or ('Overflow' in val_matrix):\n",
    "    print('\\t【VAL】Show the over-bounded matrix: \\n', val_matrix)\n",
    "else:\n",
    "    print('\\t【VAL】No element is over-bounded')\n",
    "\n",
    "# print(var_dict)\n",
    "    \n",
    "# main function to run the data generation - single process\n",
    "print(\" >> Start validate in ADS...\")\n",
    "singel_process_iteration_data_gen2h5(\n",
    "            workspace_path=workspace_path,\n",
    "            validate_dict=var_dict,\n",
    "            library_name=library_name,\n",
    "            design_name=design_name,\n",
    "            instance_name=instance_name,\n",
    "            param_range=param_range,\n",
    "            vgs_bias_param_sweep_name=vgs_bias_param_sweep_name,\n",
    "            vds_bias_param_sweep_name=vds_bias_param_sweep_name,\n",
    "            vgs_bias_simulation_name=vgs_bias_simulation_name,\n",
    "            vds_bias_simulation_name=vds_bias_simulation_name,\n",
    "            data_shape=data_shape,\n",
    "            iteration_num=1,\n",
    "            process_id=1,\n",
    "            save_path=data_save_path,\n",
    "            verilog_a_dir=verilog_a_dir)\n",
    "\n",
    "\n",
    "# plot error between prediction and original I-V data\n",
    "print(\" >> Start plot error...\")\n",
    "best_infer_index = plot_error(inference_data_path, data_save_path, inference_data_index, var_dict, val_matrix, X_goal_dict, output_csv_path)\n",
    "\n",
    "# fig 2 :: hist\n",
    "if cvae_ena and cvae_mode == 'rand' and num_sampling > 1:\n",
    "    odd = len(param_range)%2!=0\n",
    "    size  = round((len(param_range))/2)\n",
    "    fig, ax = plt.subplots(2, size, figsize=(20, 8)) \n",
    "\n",
    "    for index, key in enumerate(var_dict):\n",
    "        data = list(map(float, var_dict[key]))\n",
    "        kde = gaussian_kde(data)\n",
    "        x_vals = np.linspace(min(data), max(data), 3000)\n",
    "        y_vals = kde(x_vals)\n",
    "        coordinate = [0, index] if ((index + 1) <= size) else [1, index - size]\n",
    "        ax[coordinate[0], coordinate[1]].hist(data, bins = 30, density=True, color = 'grey', alpha = 0.6)\n",
    "        ax[coordinate[0], coordinate[1]].set_title(key)\n",
    "        ax[coordinate[0], coordinate[1]].plot(x_vals, y_vals, linewidth=2, label=\"KDE curve\", color = 'red')\n",
    "        ax[coordinate[0], coordinate[1]].grid(True, alpha=0.2)\n",
    "        ax[coordinate[0], coordinate[1]].ticklabel_format(style='sci', scilimits=(-2,3), axis='x')\n",
    "    if odd:\n",
    "        fig.delaxes(ax[-1, -1])\n",
    "    plt.tight_layout()\n",
    "\n",
    "# finnaly run the best inference case once again \n",
    "# in order to save correspondings param in ADS project file\n",
    "print('\\n >> Save the best case in ADS project...')\n",
    "best_var_dict = {k: str(v[best_infer_index]) for k, v in var_dict.items()}\n",
    "singel_process_iteration_data_gen2h5(\n",
    "            workspace_path=workspace_path,\n",
    "            validate_dict=best_var_dict,\n",
    "            library_name=library_name,\n",
    "            design_name=design_name,\n",
    "            instance_name=instance_name,\n",
    "            param_range=param_range,\n",
    "            vgs_bias_param_sweep_name=vgs_bias_param_sweep_name,\n",
    "            vds_bias_param_sweep_name=vds_bias_param_sweep_name,\n",
    "            vgs_bias_simulation_name=vgs_bias_simulation_name,\n",
    "            vds_bias_simulation_name=vds_bias_simulation_name,\n",
    "            data_shape=data_shape,\n",
    "            iteration_num=1,\n",
    "            process_id=1,\n",
    "            save_path=data_save_path,\n",
    "            verilog_a_dir=verilog_a_dir)\n",
    "\n",
    "\n",
    "# {'VOFF':'1.785', 'U0':'0.424', 'NS0ACCS':'2e+17', 'NFACTOR':'1', 'ETA0':'0.06', 'VSAT':'8e+4', 'VDSCALE':'5', 'CDSCD':'0.1', 'LAMBDA':'0.01', 'MEXPACCD':'1.5', 'DELTA':'3'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a31e359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig 2 :: hist\n",
    "if cvae_ena and cvae_mode == 'rand' and num_sampling > 1:\n",
    "    odd = len(param_range)%2!=0\n",
    "    size  = round((len(param_range))/2)\n",
    "    fig, ax = plt.subplots(2, size, figsize=(20, 8)) \n",
    "\n",
    "    for index, key in enumerate(var_dict):\n",
    "        data = list(map(float, var_dict[key]))\n",
    "        kde = gaussian_kde(data)\n",
    "        x_vals = np.linspace(min(data), max(data), 3000)\n",
    "        y_vals = kde(x_vals)\n",
    "        coordinate = [0, index] if ((index + 1) <= size) else [1, index - size]\n",
    "        ax[coordinate[0], coordinate[1]].hist(data, bins = 60, density=True, color = 'grey', alpha = 0.6)\n",
    "        ax[coordinate[0], coordinate[1]].set_title(key)\n",
    "        ax[coordinate[0], coordinate[1]].plot(x_vals, y_vals, linewidth=2, label=\"KDE curve\", color = 'red')\n",
    "        ax[coordinate[0], coordinate[1]].grid(True, alpha=0.2)\n",
    "        ax[coordinate[0], coordinate[1]].ticklabel_format(style='sci', scilimits=(-2,3), axis='x')\n",
    "    if odd:\n",
    "        fig.delaxes(ax[-1, -1])\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(\"C:\\\\Users\\\\97427\\\\Desktop\\\\figure.svg\", bbox_inches='tight', format='svg')\n",
    "    print('saved figure under path : C:\\\\Users\\\\97427\\\\Desktop\\\\figure.svg ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6085b024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig 3 :: better result representation\n",
    "\n",
    "\n",
    "ref_Output = r\"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/data_pre_processing/ref_data/meas_smoothed_Bidi_Output.h5\"\n",
    "ref_Transfer = r\"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/data_pre_processing/ref_data/meas_smoothed_Bidi_Transfer.h5\"\n",
    "X_origin_o = h5py.File(ref_Output, 'r')\n",
    "X_origin_t = h5py.File(ref_Transfer, 'r')\n",
    "X_pred = h5py.File(f\"{data_save_path}\\\\validate.h5\", 'r')\n",
    "vgs_bias_x_axis = np.linspace(-1, 5, 61)\n",
    "vds_bias_x_axis = np.linspace(-10, 0, 101)\n",
    "\n",
    "four_fig_plot(X_origin_o, X_origin_t, X_pred, vgs_bias_x_axis, vds_bias_x_axis)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
