{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ed5ea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is part of the ADS Parameter Fitting project.\n",
    "# must be used under ADS integrated Python env (A)\n",
    "# namely, ..\\ADS_install_path\\tools\\python\\python.exe --> Python 3.13.2\n",
    "# TODO: whole script is run in Jupyter because ADS python ADI only\n",
    "# supports IPython kernel !!!\n",
    "\n",
    "# packages to build DIR env\n",
    "import os, json\n",
    "# set ads dict: HPEESOF_DIR and home director : HOME\n",
    "os.environ['HPEESOF_DIR'] = 'D:/ADS/install'\n",
    "os.environ['HOME'] = 'D:/ADS/dir'\n",
    "\n",
    "# # packages to multiprocessing\n",
    "# # add current working directory\n",
    "# import sys\n",
    "# cur_path_nn = \"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting_local/IV_param_regression/NN_training\"\n",
    "# sys.path.append(cur_path_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af2452ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages to import ADS\n",
    "from keysight.ads import de\n",
    "from keysight.ads.de import db_uu as db\n",
    "from keysight.edatoolbox import ads\n",
    "import keysight.ads.dataset as dataset\n",
    "from keysight.edatoolbox import util\n",
    "from pathlib import Path\n",
    "from IPython.core import getipython\n",
    "\n",
    "# packages to import data analysis and save\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import h5py\n",
    "\n",
    "# for NN model inference\n",
    "from typing import Literal\n",
    "import subprocess \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378d7c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model inference : call func in other Python env\n",
    "\n",
    "def model_inference(python_path:str, \n",
    "                     script_dir:str, \n",
    "                     code_file_name:str, \n",
    "                     model_path:str, \n",
    "                     inference_data_path:str, \n",
    "                     inference_data_index:str,\n",
    "                     output_csv_path:str, \n",
    "                     single_input_mode:bool,\n",
    "                     cvae_ena:bool = False,\n",
    "                     cvae_mode:Literal['rand', 'mean'] = 'mean',\n",
    "                     num_sampling:int = 1) -> dict:\n",
    "    \n",
    "    if cvae_ena: # CVAE model (prability based) inference\n",
    "        if inference_data_index == 'None':\n",
    "            # single input with single a or multiple input\n",
    "            cmd = [\n",
    "                python_path, code_file_name,\n",
    "                \"--infer-run\", model_path,\n",
    "                \"--input-h5\",  inference_data_path,\n",
    "                \"--save-csv\",  output_csv_path,\n",
    "                \"--sample-mode\", cvae_mode,\n",
    "                \"--num-samples\", str(num_sampling) if cvae_mode=='rand' else '1'\n",
    "        ]\n",
    "        else:\n",
    "            cmd = [\n",
    "            python_path, code_file_name,\n",
    "            \"--infer-run\", model_path,\n",
    "            \"--input-h5\", inference_data_path,\n",
    "            \"--index\", inference_data_index,\n",
    "            \"--save-csv\", output_csv_path,\n",
    "            \"--sample-mode\", cvae_mode,\n",
    "            \"--num-samples\", str(num_sampling) if cvae_mode=='rand' else '1'\n",
    "            ]\n",
    "\n",
    "    else: # 2 stage DNN (value based) model inference\n",
    "        if inference_data_index == 'None':\n",
    "            # single input with single a or multiple input\n",
    "            cmd = [\n",
    "                python_path, code_file_name,\n",
    "                \"--data\", inference_data_path,\n",
    "                \"--infer-run\", model_path,\n",
    "                \"--input-h5\",  inference_data_path,\n",
    "                \"--save-csv\",  output_csv_path\n",
    "        ]\n",
    "        else:\n",
    "            cmd = [\n",
    "            python_path, code_file_name,\n",
    "            \"--data\", inference_data_path,\n",
    "            \"--infer-run\", model_path,\n",
    "            \"--input-h5\", inference_data_path,\n",
    "            \"--index\", inference_data_index,\n",
    "            \"--save-csv\", output_csv_path\n",
    "            ]\n",
    "\n",
    "    result = subprocess.run(cmd, cwd=script_dir, capture_output=True, text=True)\n",
    "    print(result.stdout)\n",
    "\n",
    "    # transform output from str to var dict\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.read_csv(output_csv_path)\n",
    "    if bool(inference_data_index) & single_input_mode:\n",
    "    # single input\n",
    "        df_dict = df.to_dict(orient='list')\n",
    "        var_dict = {key:str(value) for (key, value) in zip(df_dict['param'], df_dict['value'])}\n",
    "    else:\n",
    "    # Batch input\n",
    "        df_dict = df[df['index'] == 1].to_dict(orient='list')\n",
    "        df_dict.pop('index')\n",
    "        var_dict = {key:str(df_dict[key][0]) for key in df_dict}\n",
    "        \n",
    "    \n",
    "    return var_dict\n",
    "\n",
    "\n",
    "def  proxy_model_inference(python_path:str, \n",
    "                     script_dir:str, \n",
    "                     code_file_name:str, \n",
    "                     model_path:str, \n",
    "                     inference_data_path:str, \n",
    "                     inference_data_index:str,\n",
    "                     output_npy_path:str) -> np.ndarray:\n",
    "    \n",
    "    if inference_data_index == 'None':\n",
    "        # single input with single a or multiple input\n",
    "        cmd = [\n",
    "            python_path, code_file_name,\n",
    "            \"--data\", inference_data_path,\n",
    "            \"--infer-proxy-run\", model_path,\n",
    "            \"--proxy-input-h5\",  inference_data_path,\n",
    "            \"--save-xhat-npy\",  output_npy_path\n",
    "    ]\n",
    "    else:\n",
    "        cmd = [\n",
    "        python_path, code_file_name,\n",
    "        \"--data\", inference_data_path,\n",
    "        \"--infer-proxy-run\", model_path,\n",
    "        \"--proxy-input-h5\", inference_data_path,\n",
    "        \"--proxy-index\", inference_data_index,\n",
    "        \"--save-xhat-npy\", output_npy_path\n",
    "        ]\n",
    "\n",
    "    result = subprocess.run(cmd, cwd=script_dir, capture_output=True, text=True)\n",
    "    print(result.stdout)\n",
    "\n",
    "\n",
    "\n",
    "def plot_proxy_error(inference_data_path:str, x_hat_path:str, inference_data_index:str) -> None:\n",
    "    if inference_data_index != 'None':\n",
    "        x_hat = np.load(x_hat_path)[0,:]\n",
    "        x_original = h5py.File(inference_data_path, 'r')\n",
    "        x_original = x_original['X'][int(inference_data_index), :, :]\n",
    "        RMSE = np.sqrt(np.mean((x_hat - x_original) ** 2))\n",
    "        MAE = np.mean(np.abs(x_hat - x_original))\n",
    "        NMAE = MAE / np.mean(x_original) *100\n",
    "\n",
    "        Vds_range = np.arange(-3.5, 8.5 + 0.1, 0.1)\n",
    "\n",
    "        # plot validation result\n",
    "        # legend_values = np.linspace(1.5, 6.5, 6)  \n",
    "        legend_values = np.linspace(1, 7, 7)  \n",
    "        # create colormap\n",
    "        cmap = plt.cm.winter\n",
    "        norm = plt.Normalize(vmin=legend_values.min(), vmax=legend_values.max())\n",
    "\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(15, 8)) \n",
    "        for i in range(7):\n",
    "            color = cmap(norm(legend_values[i]))\n",
    "            ax[0].plot( Vds_range, x_original[i, :], label=f\"VGS={i+1} V\", color = color)\n",
    "            ax[1].plot( Vds_range, x_hat[i, :], label=f\"VGS={i+1} V\", color = color)\n",
    "            ax[2].plot( Vds_range, x_hat[i, :] - x_original[i, :], label=f\"VGS={i+1} V\", color = color)\n",
    "\n",
    "        ax[0].legend()\n",
    "        ax[0].grid(True)\n",
    "        ax[0].set_title('original I-V (generated, with noise)')\n",
    "        ax[0].set_xlabel(\"VDS (V)\")\n",
    "        ax[0].set_ylabel(\"IDS (A)\")\n",
    "        ax[1].grid(True)\n",
    "        ax[1].legend()\n",
    "        ax[1].set_title('predicted I-V (proxy g, DNN based)')\n",
    "        ax[1].set_xlabel(\"VDS (V)\")\n",
    "        ax[1].set_ylabel(\"IDS (A)\")\n",
    "        ax[2].grid(True)\n",
    "        ax[2].set_title(f'error of both I-V \\n average MAE = {MAE:.3f} \\n nMAE = {NMAE:.3f}%')\n",
    "        ax[2].set_xlabel(\"VDS (V)\")\n",
    "        ax[2].set_ylabel(\"IDS (A)\")\n",
    "\n",
    "        # add color bar\n",
    "        sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "        sm.set_array([])  \n",
    "        cbar = fig.colorbar(sm, ax=ax)\n",
    "        cbar.set_label(\"VGS (V)\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        assert ValueError(\"No index provided, cannot plot proxy error for multiple samples.\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def plot_error(inference_data_path:str, data_save_path:str, inference_data_index:str, infer_param_dict: dict, check_list: list[str]):\n",
    "    # extract data from both pred and validation\n",
    "    X_orginal = h5py.File(inference_data_path, 'r')\n",
    "    X_pred = h5py.File(f\"{data_save_path}\\\\validate.h5\", 'r')\n",
    "    if inference_data_index != 'None':\n",
    "        index = int(inference_data_index) \n",
    "        X_orginal_iv = X_orginal['X'][index, :, :]\n",
    "        X_orginal_y = X_orginal['Y'][index, :, :]\n",
    "    else:\n",
    "        if len(X_orginal['X'].shape) == 2:\n",
    "            X_orginal_iv = X_orginal['X'][:]\n",
    "        else:\n",
    "            X_orginal_iv = X_orginal['X'][0,:]\n",
    "\n",
    "\n",
    "    X_pred_iv = X_pred['X_iv'][:] if X_pred['X_iv'][:].shape[0]==7 else X_pred['X_iv'][0, :, :]\n",
    "    \n",
    "\n",
    "    if inference_data_index != 'None':\n",
    "        # print error of predicted paramters\n",
    "        print('Relative errors of predicted paramters:')\n",
    "        for index,key in enumerate(infer_param_dict):\n",
    "            val = float(infer_param_dict[key])  \n",
    "            ref = float(X_orginal_y[index][0])    \n",
    "            if check_list[index] != 'ok':\n",
    "                print(f'{key:<10}: {(val-ref)/ref*100:>10.2f}% ({check_list[index]})')\n",
    "            else:   \n",
    "                print(f'{key:<10}: {(val-ref)/ref*100:>10.2f}%')\n",
    "\n",
    "    Vds_range = np.arange(-3.5, 8.5 + 0.1, 0.1)\n",
    "\n",
    "    # plot validation result\n",
    "    # legend_values = np.linspace(1.5, 6.5, 6)  \n",
    "    legend_values = np.linspace(1, 7, 7)  \n",
    "    # create colormap\n",
    "    cmap = plt.cm.winter\n",
    "    norm = plt.Normalize(vmin=legend_values.min(), vmax=legend_values.max())\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 8)) \n",
    "    RMSE = np.mean((X_pred_iv - X_orginal_iv) ** 2)\n",
    "    MAE = np.mean(np.abs(X_pred_iv - X_orginal_iv))\n",
    "    NMAE = MAE / np.mean(X_orginal_iv) *100\n",
    "    for i in range(7):\n",
    "        color = cmap(norm(legend_values[i]))\n",
    "        ax[0].plot( Vds_range, X_orginal_iv[i, :], label=f\"VGS={i+1} V\", color = color)\n",
    "        ax[1].plot( Vds_range, X_pred_iv[i, :], label=f\"VGS={i+1} V\", color = color)\n",
    "        ax[2].plot( Vds_range, X_pred_iv[i, :] - X_orginal_iv[i, :], label=f\"VGS={i+1} V\", color = color)\n",
    "\n",
    "    ax[0].legend()\n",
    "    ax[0].grid(True)\n",
    "    ax[0].set_title('original I-V (generated, with noise)')\n",
    "    ax[0].set_xlabel(\"VDS (V)\")\n",
    "    ax[0].set_ylabel(\"IDS (A)\")\n",
    "    ax[1].grid(True)\n",
    "    ax[1].legend()\n",
    "    ax[1].set_title('predicted I-V (NN based)')\n",
    "    ax[1].set_xlabel(\"VDS (V)\")\n",
    "    ax[1].set_ylabel(\"IDS (A)\")\n",
    "    ax[2].grid(True)\n",
    "    ax[2].set_title(f'error of both I-V \\n average MAE = {MAE:.3f} \\n nMAE = {NMAE:.3f}%')\n",
    "    ax[2].set_xlabel(\"VDS (V)\")\n",
    "    ax[2].set_ylabel(\"IDS (A)\")\n",
    "\n",
    "    # add color bar\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])  \n",
    "    cbar = fig.colorbar(sm, ax=ax)\n",
    "    cbar.set_label(\"VGS (V)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5189c712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary class and functions for ADS simulation\n",
    "\n",
    "class PyADS():\n",
    "    def __init__(self):\n",
    "        self.HPEESOF_DIR = 'D:/ADS/install'\n",
    "        self.HOME = 'D:/ADS/dir'\n",
    "        self.cur_workspace_path = None\n",
    "        self.workspace = None\n",
    "        self.cur_library_name = None\n",
    "        self.library = None\n",
    "        self.cur_design_name = None\n",
    "        self.design = None\n",
    "\n",
    "    def create_and_open_an_empty_workspace(self, workspace_path: str):\n",
    "    # example : workspace_path = \"C:/ADS_Python_Tutorials/tutorial1_wrk\"\n",
    "    # Ensure there isn't already a workspace open\n",
    "        if de.workspace_is_open():\n",
    "            de.close_workspace()\n",
    "    \n",
    "        # Cannot create a workspace if the directory already exists\n",
    "        if os.path.exists(workspace_path):\n",
    "            raise RuntimeError(f\"Workspace directory already exists: {workspace_path}\")\n",
    "    \n",
    "        # Create the workspace\n",
    "        workspace = de.create_workspace(workspace_path)\n",
    "        # Open the workspace\n",
    "        workspace.open()\n",
    "        # Return the open workspace and close when it finished\n",
    "        return workspace\n",
    "    \n",
    "    def create_a_library_and_add_it_to_the_workspace(self, workspace: de.Workspace, library_name: str) -> None:\n",
    "        #assert workspace.path is not None\n",
    "        # Libraries can only be added to an open workspace\n",
    "        assert workspace.is_open\n",
    "        # We'll create a library in the directory of the workspace\n",
    "        library_path = workspace.path / library_name\n",
    "        # Create the library\n",
    "        de.create_new_library(library_name, library_path)\n",
    "        # And add it to the workspace (update lib.defs)\n",
    "        workspace.add_library(library_name, library_path, de.LibraryMode.SHARED)\n",
    "        lib=workspace.open_library(library_name,library_path,de.LibraryMode.SHARED)\n",
    "        return lib\n",
    "\n",
    "    def schematic_simulation(self, workspace_path: str, library_name: str, design_name: str, instance_name: str, var_dict: dict, vgs_bias_param_sweep_name: str, vds_bias_param_sweep_name: str, vgs_bias_simulation_name: str, vds_bias_simulation_name: str) -> None:\n",
    "        ''' Load Path and files, Edit the design variables, Simulate the design, and return the dataset '''\n",
    "\n",
    "        # >> Load Path and files\n",
    "        if not os.path.exists(workspace_path):\n",
    "            raise RuntimeError(f\"Workspace directory doesn't exist: {workspace_path}\")\n",
    "        if de.workspace_is_open():\n",
    "            de.close_workspace()\n",
    "        \n",
    "        # Open the workspace\n",
    "        # if (not self.workspace) or (self.cur_workspace_path != workspace_path):\n",
    "        self.workspace = de.open_workspace(workspace_path)\n",
    "        self.cur_workspace_path = workspace_path\n",
    "        # Open the library\n",
    "        # if (not self.library) or (self.cur_library_name != library_name):\n",
    "        self.library = self.workspace.open_library(lib_name=library_name, mode=de.LibraryMode.SHARED)\n",
    "        self.cur_library_name = library_name\n",
    "        # Open the design\n",
    "        # if (not self.design) or (self.cur_design_name != design_name):\n",
    "        self.design = db.open_design((library_name, design_name, \"schematic\"), db.DesignMode.APPEND)\n",
    "        self.cur_design_name = design_name\n",
    "\n",
    "        # >> Edit the design variables\n",
    "        # edit VAR\n",
    "        v = self.design.get_instance(inst_name=instance_name)\n",
    "        assert v.is_var_instance\n",
    "        for var_name in var_dict:\n",
    "            v.vars[var_name] = var_dict[var_name]\n",
    "        # Save the design\n",
    "        self.design.save_design()\n",
    "        # Simulate the design\n",
    "        output_dir = os.path.join(self.workspace.path, \"output\")\n",
    "        netlist_file = os.path.join(output_dir, \"data_gen.ckt\")\n",
    "        output_file =  os.path.join(output_dir, \"data_gen.ckt.out\")\n",
    "        # create the simulation output directory\n",
    "        util.safe_makedirs(output_dir)\n",
    "\n",
    "        # >> Simulate and return the dataset\n",
    "        ipython = getipython.get_ipython()\n",
    "        if ipython is None:\n",
    "            print(\"The remaining portion of the script must be run in an IPython environment. Exiting.\")\n",
    "            return\n",
    "        # capture the netlist in a string\n",
    "        netlist = self.design.generate_netlist()\n",
    "        # access to the simulator object to run netlists\n",
    "        simulator = ads.CircuitSimulator()\n",
    "        # run the netlist, this will block output\n",
    "        simulator.run_netlist(netlist, output_dir=output_dir, netlist_file=netlist_file, output_file=output_file)\n",
    "        output_data = dataset.open(Path(os.path.join(output_dir, f\"{design_name}.ds\")))\n",
    "        \n",
    "        # >> return data in pandas DataFrame format\n",
    "        # <class 'pandas.core.frame.DataFrame'>\n",
    "        data_ids_vds = output_data[f'{vgs_bias_param_sweep_name}.{vgs_bias_simulation_name}.DC'].to_dataframe().reset_index()\n",
    "        data_ids_vgs = output_data[f'aele_0.{vds_bias_param_sweep_name}.{vds_bias_simulation_name}'].to_dataframe().reset_index()\n",
    "        return data_ids_vds, data_ids_vgs\n",
    "    \n",
    "\n",
    "\n",
    "    def dataset_reshape(self, pd_data_IV: pd.DataFrame, pd_data_gm: pd.DataFrame, IV_dimension: list, gm_dimension: list, var_dict: dict):\n",
    "        ''' reshape the dataset into desired input matrix and output vector '''\n",
    "        IV_row_count = IV_dimension[0] # Vgs\n",
    "        IV_col_count = IV_dimension[1] # Vds\n",
    "        gm_row_count = gm_dimension[0] # Vds\n",
    "        gm_col_count = gm_dimension[1] # Vgs\n",
    "\n",
    "        output_x_IV = np.empty((IV_row_count, IV_col_count),dtype=np.float64)\n",
    "        output_x_gm = np.empty((gm_row_count, gm_col_count),dtype=np.float64)\n",
    "        output_y = np.empty((len(var_dict), 1),dtype=np.float64)\n",
    "\n",
    "        for row in range(IV_row_count):\n",
    "            output_x_IV[row, :] = pd_data_IV.loc[pd_data_IV['VGS'] == (row + 1), 'IDS.i'].to_numpy()\n",
    "        for col in range(gm_col_count):\n",
    "            output_x_gm[:, col] = pd_data_gm.loc[pd_data_gm['VGS'] == (col + 1.5), 'gm'].to_numpy()\n",
    "        for index, item in enumerate(var_dict):\n",
    "            output_y[index, 0] = var_dict[item]\n",
    "\n",
    "        return output_x_IV, output_x_gm, output_y\n",
    "    \n",
    "\n",
    "def param_random_generator(param_range: dict):\n",
    "    ''' generate a random parameter set for the HEMT model '''\n",
    "    # define the parameter range\n",
    "    # param_range = {\n",
    "    #     'VOFF': (-1.2, 2.6),\n",
    "    #     'U0': (0, 2.2),\n",
    "    #     'NS0ACCS': (1e15, 1e20),\n",
    "    #     'NFACTOR': (0.1, 5),\n",
    "    #     'ETA0': (0, 1),\n",
    "    #     'VSAT': (5e4, 1e7),\n",
    "    #     'VDSCALE': (0.5, 1e6),\n",
    "    #     'CDSCD': (1e-5, 0.75),\n",
    "    #     'LAMBDA': (0, 0.2),\n",
    "    #     'MEXPACCD': (0.05, 12),\n",
    "    #     'DELTA': (2, 100)\n",
    "    # }\n",
    "    # generate random parameters\n",
    "    var_dict = {key: str(np.random.uniform(low=val[0], high=val[1])) for key, val in param_range.items()}\n",
    "    return var_dict\n",
    "\n",
    "def init_h5_file(h5_path, x_iv_shape, x_gm_shape, y_shape,\n",
    "                 dtype_x=np.float64, dtype_y=np.float64):\n",
    "    with h5py.File(h5_path, 'w') as f:\n",
    "        # X: [num_samples, m, n]\n",
    "        f.create_dataset(\n",
    "            'X_iv',\n",
    "            shape=(0, x_iv_shape[0], x_iv_shape[1]),\n",
    "            maxshape=(None, x_iv_shape[0], x_iv_shape[1]),\n",
    "            dtype=dtype_x\n",
    "        )\n",
    "        f.create_dataset(\n",
    "            'X_gm',\n",
    "            shape=(0, x_gm_shape[0], x_gm_shape[1]),\n",
    "            maxshape=(None, x_gm_shape[0], x_gm_shape[1]),\n",
    "            dtype=dtype_x\n",
    "        )\n",
    "        # Y: [num_samples, y_len]\n",
    "        f.create_dataset(\n",
    "            'Y',\n",
    "            shape=(0, y_shape[0], 1),\n",
    "            maxshape=(None, y_shape[0], 1),\n",
    "            dtype=dtype_y\n",
    "        )\n",
    "\n",
    "def append_to_h5(h5_path, x_iv_new, x_gm_new, y_new):\n",
    "    x_iv_new = np.asarray(x_iv_new, dtype=np.float64)\n",
    "    x_gm_new = np.asarray(x_gm_new, dtype=np.float64)\n",
    "    y_new = np.asarray(y_new, dtype=np.float64)\n",
    "\n",
    "    # 确保 y_new 是二维 (batch_size, y_len)\n",
    "    if y_new.ndim == 2:\n",
    "        y_new = y_new.reshape(-1, 1)\n",
    "    else:\n",
    "        raise RuntimeError(f\"y_new must be vector, but got shape {y_new.shape}\")\n",
    "\n",
    "    with h5py.File(h5_path, 'a') as f:\n",
    "        ds_x_iv = f['X_iv']\n",
    "        ds_x_gm = f['X_gm']\n",
    "        ds_y = f['Y']\n",
    "\n",
    "        cur_len = ds_x_iv.shape[0]\n",
    "        new_len = cur_len + 1\n",
    "\n",
    "        # 扩展\n",
    "        ds_x_iv.resize(new_len, axis=0)\n",
    "        ds_x_gm.resize(new_len, axis=0)\n",
    "        ds_y.resize(new_len, axis=0)\n",
    "\n",
    "        # 赋值\n",
    "        ds_x_iv[cur_len:new_len, :, :] = x_iv_new\n",
    "        ds_x_gm[cur_len:new_len, :, :] = x_gm_new\n",
    "        ds_y[cur_len:new_len, :] = y_new\n",
    "\n",
    "\n",
    "def singel_process_iteration_data_gen2h5(workspace_path: str, validate_dict: dict, library_name: str, design_name: str, instance_name: str, param_range: dict, vgs_bias_param_sweep_name: str, vds_bias_param_sweep_name: str, vgs_bias_simulation_name: str, vds_bias_simulation_name: str, save_path: str, iteration_num: int  = 1, process_id: int = 1, dtype_x=np.float64, dtype_y=np.float64):\n",
    "    ''' generate dataset in single process iteration '''\n",
    "    # create an instance of the PyADS class\n",
    "    ads_ctrl = PyADS()\n",
    "    X_iv_shape = [7,121]\n",
    "    X_gm_shape = [121,6]\n",
    "    Y_shape = [11,1]\n",
    "\n",
    "    # init h5 file\n",
    "    if not validate_dict:\n",
    "        init_h5_file(f\"{save_path}\\\\dataset_process_{process_id}.h5\", X_iv_shape, X_gm_shape, Y_shape)\n",
    "    else:\n",
    "        init_h5_file(f\"{save_path}\\\\validate.h5\", X_iv_shape, X_gm_shape, Y_shape)\n",
    "\n",
    "    for i in range(iteration_num):\n",
    "        start_time = time.time()\n",
    "        if validate_dict:\n",
    "            var_dict = validate_dict\n",
    "        else:\n",
    "            var_dict = param_random_generator(param_range)\n",
    "        pd_data_vgs_bias, pd_data_gm = ads_ctrl.schematic_simulation(\n",
    "            workspace_path,\n",
    "            library_name,\n",
    "            design_name,\n",
    "            instance_name,\n",
    "            var_dict,\n",
    "            vgs_bias_param_sweep_name,\n",
    "            vds_bias_param_sweep_name,\n",
    "            vgs_bias_simulation_name,\n",
    "            vds_bias_simulation_name\n",
    "        )\n",
    "        X_iv, X_gm, y = ads_ctrl.dataset_reshape(pd_data_vgs_bias, pd_data_gm, X_iv_shape, X_gm_shape, var_dict)\n",
    "        end_time = time.time()\n",
    "        if not validate_dict:\n",
    "            print(f' >> Process {process_id} :: Loop {i + 1}/{iteration_num} :: used time:', round(end_time - start_time, 2), 's')\n",
    "        else:\n",
    "            print(f'finish validation in ADS :: used time:', round(end_time - start_time, 2), 's')\n",
    "\n",
    "        try:\n",
    "            if not validate_dict:\n",
    "                append_to_h5(f\"{save_path}\\\\dataset_process_{process_id}.h5\", X_iv, X_gm, y)\n",
    "            else:\n",
    "                append_to_h5(f\"{save_path}\\\\validate.h5\", X_iv, X_gm, y)\n",
    "        except:\n",
    "            print(f\"【ERROR】Error appending data in process {process_id} at iteration {i + 1}.\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e163630e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ADS\n",
    "# DEFINE VARIABLES\n",
    "\n",
    "workspace_path = \"E:\\\\personal_Data\\\\Document of School\\\\Uni Stuttgart\\\\Masterarbeit\\\\Simulation\\\\ADS\\\\ASM_HEMT1_wrk_1_Jia\"\n",
    "validate_dict = None\n",
    "library_name = \"IAF_pGaN_lib\"\n",
    "design_name = \"gs66508bv1_Pytest_simple_paramset\"\n",
    "instance_name = \"IV\"\n",
    "var_dict_default = {'VOFF':'1.785', 'U0':'0.424', 'NS0ACCS':'2e+17', 'NFACTOR':'1', 'ETA0':'0.06', 'VSAT':'8e+4', 'VDSCALE':'5', 'CDSCD':'0.1', 'LAMBDA':'0.01', 'MEXPACCD':'1.5', 'DELTA':'3'}\n",
    "param_range = {\n",
    "        'VOFF': (-1.2, 2.6),\n",
    "        'U0': (0, 2.2),\n",
    "        'NS0ACCS': (1e15, 1e20),\n",
    "        'NFACTOR': (0.1, 5),\n",
    "        'ETA0': (0, 1),\n",
    "        'VSAT': (5e4, 1e7),\n",
    "        'VDSCALE': (0.5, 1e6),\n",
    "        'CDSCD': (1e-5, 0.75),\n",
    "        'LAMBDA': (0, 0.2),\n",
    "        'MEXPACCD': (0.05, 12),\n",
    "        'DELTA': (2, 100)\n",
    "    }\n",
    "vgs_bias_param_sweep_name = 'Sweep_vgs'\n",
    "vds_bias_param_sweep_name = 'Sweep_vds'\n",
    "vgs_bias_simulation_name = 'DC1'\n",
    "vds_bias_simulation_name = 'DC2'\n",
    "iteration_num = 1000\n",
    "process_id = 1\n",
    "data_save_path = \"E:\\\\personal_Data\\\\Document of School\\\\Uni Stuttgart\\\\Masterarbeit\\\\Code\\\\param_regression\\\\ADS_Parameter_Fitting\\\\IV_param_regression\\\\NN_training\\\\dataset\\\\temp_generated\"\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# For NN model inference\n",
    "\n",
    "python_path = r\"D:/Miniconda/envs/DL/python.exe\"  # model based python path\n",
    "script_dir  = r\"E:\\\\personal_Data\\\\Document of School\\\\Uni Stuttgart\\\\Masterarbeit\\\\Code\\\\param_regression\\\\ADS_Parameter_Fitting\\\\IV_param_regression\\\\NN_training\" # path of code\n",
    "\n",
    "training_data_path   = r\"E:\\\\personal_Data\\\\Document of School\\\\Uni Stuttgart\\\\Masterarbeit\\\\Code\\\\param_regression\\\\ADS_Parameter_Fitting\\\\IV_param_regression\\\\NN_training\\\\dataset\\\\training\\\\stage_x_ml_full_data.h5\"  # path of training dataset\n",
    "\n",
    "meas_like_val_set = r\"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/NN_training/dataset/training/stage_2_ml_data.h5\" \n",
    "\n",
    "meas_path = r\"E:\\\\personal_Data\\\\Document of School\\\\Uni Stuttgart\\\\Masterarbeit\\\\Code\\\\param_regression\\\\ADS_Parameter_Fitting\\\\IV_param_regression\\\\NN_training\\\\dataset\\\\training\\\\meas_data.h5\"\n",
    "\n",
    "output_csv_path = r\"E:\\\\personal_Data\\\\Document of School\\\\Uni Stuttgart\\\\Masterarbeit\\\\Code\\\\param_regression\\\\ADS_Parameter_Fitting\\\\IV_param_regression\\\\NN_training\\\\dataset\\\\temp_generated\\\\pred_7row.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e2b2cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> Start model inference...\n",
      "Using CUDA device 0: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "\n",
      "--- Solution Statistics (for first input) ---\n",
      "  VOFF     : mean=1.06  std=nan\n",
      "  U0       : mean=-1.132  std=nan\n",
      "  NS0ACCS  : mean=-0.702  std=nan\n",
      "  NFACTOR  : mean=-0.001651  std=nan\n",
      "  ETA0     : mean=-0.877  std=nan\n",
      "  VSAT     : mean=0.4307  std=nan\n",
      "  VDSCALE  : mean=-0.8742  std=nan\n",
      "  CDSCD    : mean=-0.7956  std=nan\n",
      "  LAMBDA   : mean=0.2813  std=nan\n",
      "  MEXPACCD : mean=0.1187  std=nan\n",
      "  DELTA    : mean=0.1267  std=nan\n",
      "\n",
      "Saved all 1 sampled solutions to E:\\\\personal_Data\\\\Document of School\\\\Uni Stuttgart\\\\Masterarbeit\\\\Code\\\\param_regression\\\\ADS_Parameter_Fitting\\\\IV_param_regression\\\\NN_training\\\\dataset\\\\temp_generated\\\\pred_7row.csv\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'param'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Model inference\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m >> Start model inference...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m var_dict = \u001b[43mmodel_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpython_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpython_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscript_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscript_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcode_file_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43masm_hemt_2stage_dnn.py\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcvae_ena\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43masm_hemt_cvae.py\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43minference_data_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43minference_data_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43minference_data_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43minference_data_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_csv_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_csv_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43msingle_input_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43msingle_input_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcvae_ena\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcvae_ena\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcvae_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcvae_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_sampling\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_sampling\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# range correction\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m >> Start output range correction...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mmodel_inference\u001b[39m\u001b[34m(python_path, script_dir, code_file_name, model_path, inference_data_path, inference_data_index, output_csv_path, single_input_mode, cvae_ena, cvae_mode, num_sampling)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(inference_data_index) & single_input_mode:\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# single input\u001b[39;00m\n\u001b[32m     66\u001b[39m     df_dict = df.to_dict(orient=\u001b[33m'\u001b[39m\u001b[33mlist\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     var_dict = {key:\u001b[38;5;28mstr\u001b[39m(value) \u001b[38;5;28;01mfor\u001b[39;00m (key, value) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mdf_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mparam\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m, df_dict[\u001b[33m'\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m'\u001b[39m])}\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Batch input\u001b[39;00m\n\u001b[32m     70\u001b[39m     df_dict = df[df[\u001b[33m'\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m'\u001b[39m] == \u001b[32m1\u001b[39m].to_dict(orient=\u001b[33m'\u001b[39m\u001b[33mlist\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'param'"
     ]
    }
   ],
   "source": [
    "'''single model inference'''\n",
    "\n",
    "\n",
    "# model_path = r\"E:\\\\personal_Data\\\\Document of School\\\\Uni Stuttgart\\\\Masterarbeit\\\\Code\\\\param_regression\\\\ADS_Parameter_Fitting\\\\IV_param_regression\\\\NN_training\\\\runs_test\\\\log_dataset_test\\\\2_4_0_1_normal_log\"\n",
    "\n",
    "model_path = r\"E:\\\\personal_Data\\\\Document of School\\\\Uni Stuttgart\\\\Masterarbeit\\\\Code\\\\param_regression\\\\ADS_Parameter_Fitting\\\\IV_param_regression\\\\NN_training\\\\temp_2\\\\version_2_1\"\n",
    "\n",
    "\n",
    "# basic params\n",
    "inference_data_path = training_data_path\n",
    "inference_data_index = \"7\" #  from 0 to 9999\n",
    "# \"None\"    : for 1. single input with shape (2,6,121)\n",
    "#                 2. multile input with shaple (N,2,6,121)\n",
    "# str(int)  : for single input with shape (N,2,6,121)\n",
    "single_input_mode = bool(True)\n",
    "cvae_ena = True\n",
    "cvae_mode = 'mean'\n",
    "num_sampling = 20 # only used when cvae_mode = 'rand'\n",
    "\n",
    "# Model inference\n",
    "print(\" >> Start model inference...\")\n",
    "var_dict = model_inference(\n",
    "    python_path=python_path,\n",
    "    script_dir=script_dir,\n",
    "    code_file_name=\"asm_hemt_2stage_dnn.py\" if not cvae_ena else \"asm_hemt_cvae.py\",\n",
    "    model_path=model_path,\n",
    "    inference_data_path=inference_data_path,\n",
    "    inference_data_index=inference_data_index,\n",
    "    output_csv_path=output_csv_path,\n",
    "    single_input_mode=single_input_mode,\n",
    "    cvae_ena=cvae_ena,\n",
    "    cvae_mode=cvae_mode,\n",
    "    num_sampling=num_sampling\n",
    ")\n",
    "\n",
    "# range correction\n",
    "print(\" >> Start output range correction...\")\n",
    "invalid_list = ['ok']*len(param_range)\n",
    "for index,key in enumerate(param_range):\n",
    "    (low,high) = param_range[key]\n",
    "    if float(var_dict[key]) < low:\n",
    "        var_dict[key] = str(low)\n",
    "        invalid_list[index] = 'Underflow'\n",
    "    elif float(var_dict[key]) > high:\n",
    "        var_dict[key] = str(high)\n",
    "        invalid_list[index] = 'Overflow'\n",
    "\n",
    "\n",
    "# main function to run the data generation - single process\n",
    "print(\" >> Start validate in ADS...\")\n",
    "singel_process_iteration_data_gen2h5(\n",
    "    workspace_path=workspace_path,\n",
    "    validate_dict=var_dict,\n",
    "    library_name=library_name,\n",
    "    design_name=design_name,\n",
    "    instance_name=instance_name,\n",
    "    param_range=param_range,\n",
    "    vgs_bias_param_sweep_name=vgs_bias_param_sweep_name,\n",
    "    vds_bias_param_sweep_name=vds_bias_param_sweep_name,\n",
    "    vgs_bias_simulation_name=vgs_bias_simulation_name,\n",
    "    vds_bias_simulation_name=vds_bias_simulation_name,\n",
    "    save_path=data_save_path)\n",
    "\n",
    "\n",
    "# plot error between prediction and original I-V data\n",
    "print(\" >> Start plot error...\")\n",
    "plot_error(inference_data_path, data_save_path, inference_data_index, var_dict, invalid_list)\n",
    "\n",
    "# {'VOFF':'1.785', 'U0':'0.424', 'NS0ACCS':'2e+17', 'NFACTOR':'1', 'ETA0':'0.06', 'VSAT':'8e+4', 'VDSCALE':'5', 'CDSCD':'0.1', 'LAMBDA':'0.01', 'MEXPACCD':'1.5', 'DELTA':'3'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c43c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ensemble Model Inference'''\n",
    "\n",
    "model_path_prefix = r\"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/NN_training/runs_test/2stage_ensemble/version_\"\n",
    "ensemble_num = 2\n",
    "\n",
    "# basic params\n",
    "inference_data_path = meas_path\n",
    "inference_data_index = \"None\" #  from 0 to 9999\n",
    "# \"None\"    : for 1. single input with shape (2,6,121)\n",
    "#                 2. multile input with shaple (N,2,6,121)\n",
    "# str(int)  : for single input with shape (N,2,6,121)\n",
    "single_input_mode = bool(True)\n",
    "\n",
    "\n",
    "# Model inference\n",
    "print(\" >> Start ensemble model inference...\")\n",
    "var_dict = [dict()]*ensemble_num\n",
    "var_dict_ensemble = {}\n",
    "for i in range(ensemble_num):\n",
    "    print(f\" >> Start model inference {i+1}...\")\n",
    "    model_temp_path = f\"{model_path_prefix}{i+1}\"\n",
    "    var_dict[i] = model_inference(\n",
    "        python_path=python_path,\n",
    "        script_dir=script_dir,\n",
    "        code_file_name=code_file_name,\n",
    "        model_path=model_temp_path,\n",
    "        inference_data_path=inference_data_path,\n",
    "        inference_data_index=inference_data_index,\n",
    "        output_csv_path=output_csv_path,\n",
    "        single_input_mode=single_input_mode\n",
    "    )\n",
    "\n",
    "for index,key in enumerate(param_range):\n",
    "    var_dict_ensemble[key] = str(float(np.mean([float(var_dict[i][key]) for i in range(ensemble_num)])))\n",
    "\n",
    "print(\" >> Ensemble model inference results:\")\n",
    "for key in var_dict_ensemble:\n",
    "    print(f\"{key:<10}: {float(var_dict_ensemble[key]):>12.4e}\")\n",
    "print('\\n')\n",
    "\n",
    "# main function to run the data generation - single process\n",
    "print(\" >> Start validate in ADS for ensemble model...\")\n",
    "singel_process_iteration_data_gen2h5(\n",
    "    workspace_path=workspace_path,\n",
    "    validate_dict=var_dict_ensemble,\n",
    "    library_name=library_name,\n",
    "    design_name=design_name,\n",
    "    instance_name=instance_name,\n",
    "    param_range=param_range,\n",
    "    vgs_bias_param_sweep_name=vgs_bias_param_sweep_name,\n",
    "    vds_bias_param_sweep_name=vds_bias_param_sweep_name,\n",
    "    vgs_bias_simulation_name=vgs_bias_simulation_name,\n",
    "    vds_bias_simulation_name=vds_bias_simulation_name,\n",
    "    save_path=data_save_path)\n",
    "\n",
    "\n",
    "# plot error between prediction and original I-V data\n",
    "print(\" >> Start plot error for ensemble model...\")\n",
    "invalid_list = ['ok']*len(param_range)\n",
    "plot_error(inference_data_path, data_save_path, inference_data_index, var_dict, invalid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d09695",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"proxy model G inference -> asm-hemt model\"\n",
    "\n",
    "python_path = r\"D:/Miniconda/envs/DL/python.exe\"  # model based python path\n",
    "script_dir  = r\"E:\\\\personal_Data\\\\Document of School\\\\Uni Stuttgart\\\\Masterarbeit\\\\Code\\\\param_regression\\\\ADS_Parameter_Fitting\\\\IV_param_regression\\\\NN_training\" # path of code\n",
    "code_file_name = \"asm_hemt_2stage_dnn.py\"\n",
    "\n",
    "\n",
    "meas_like_val_set = r\"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/NN_training/dataset/training/stage_2_ml_data.h5\"  # path of meas like dataset\n",
    "\n",
    "g_training_set = r\"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/NN_training/dataset/training/stage_x_ml_full_data.h5\" # path of training dataset\n",
    "\n",
    "inference_data_path = g_training_set\n",
    "meas_path = r\"E:/personal_Data/Document of School/Uni Stuttgart/Masterarbeit/Code/param_regression/ADS_Parameter_Fitting/IV_param_regression/NN_training/dataset/training/meas_data.h5\"\n",
    "\n",
    "output_npy_path = r\"E:\\\\personal_Data\\\\Document of School\\\\Uni Stuttgart\\\\Masterarbeit\\\\Code\\\\param_regression\\\\ADS_Parameter_Fitting\\\\IV_param_regression\\\\NN_training\\\\dataset\\\\temp_generated\\\\xhat.npy\"\n",
    "\n",
    "model_path = r\"E:\\\\personal_Data\\\\Document of School\\\\Uni Stuttgart\\\\Masterarbeit\\\\Code\\\\param_regression\\\\ADS_Parameter_Fitting\\\\IV_param_regression\\\\NN_training\\\\model\\\\runs_2stage_3rd_version\\\\proxy_g\\\\best_g_log_bigger\"\n",
    "\n",
    "inference_data_index = '8526'\n",
    "\n",
    "proxy_model_inference(python_path = python_path, \n",
    "                     script_dir = script_dir, \n",
    "                     code_file_name = code_file_name, \n",
    "                     model_path = model_path, \n",
    "                     inference_data_path = inference_data_path, \n",
    "                     inference_data_index = inference_data_index,\n",
    "                     output_npy_path = output_npy_path)\n",
    "\n",
    "plot_proxy_error(inference_data_path, \n",
    "                 output_npy_path, \n",
    "                 inference_data_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
